{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Runner_SNLI.ipynb","provenance":[],"authorship_tag":"ABX9TyO5tySBWYaSEYTWdNw8xul+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"6087f8c8c090440fb72b605abf3a9ef8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7671ac6a78e640dbbdd26a4c9e36055a","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f580e65976524bfa8f8fa8e1e645bdef","IPY_MODEL_45548de77288480290e7dc884582d4cc","IPY_MODEL_dddc0ccf4e7f485bb874791f761ecf00"]}},"7671ac6a78e640dbbdd26a4c9e36055a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f580e65976524bfa8f8fa8e1e645bdef":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_98272c68c1854032a565837a10625a57","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5bcf46146ddc46498c231bf23e81f478"}},"45548de77288480290e7dc884582d4cc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4d40297703934b18ac2ddda8494311b6","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":597257159,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":597257159,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8340d1f90e2640b189122a0e5074e7ec"}},"dddc0ccf4e7f485bb874791f761ecf00":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_70a7a00ef4a540ac8c01804326e4d5af","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 570M/570M [00:18&lt;00:00, 32.0MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f61c75d28ee74a0ebecbba0988a99702"}},"98272c68c1854032a565837a10625a57":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5bcf46146ddc46498c231bf23e81f478":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4d40297703934b18ac2ddda8494311b6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"8340d1f90e2640b189122a0e5074e7ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"70a7a00ef4a540ac8c01804326e4d5af":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f61c75d28ee74a0ebecbba0988a99702":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"20371a9a0dfc4252b2e6841b65941c9b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_cabce981c2104b1caf745e7ac672afc2","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d61045e7d2a2421abdd67e2aec16a9fd","IPY_MODEL_ea0441251d85468ea4dc6806fe8bb39c","IPY_MODEL_cf121efe0b294a2990bc82f0e1b7aead"]}},"cabce981c2104b1caf745e7ac672afc2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d61045e7d2a2421abdd67e2aec16a9fd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3cdea71db51e420388ba08b5fc139ec7","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: ","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_13150a9debcb4f3f8b36c47d8b2282b1"}},"ea0441251d85468ea4dc6806fe8bb39c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_3aa1f79c730145319d4b49fede629634","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1411,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1411,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ad8ed5ec45ef4bd8867bd90267e18f48"}},"cf121efe0b294a2990bc82f0e1b7aead":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8ff6bd0821a44233a8b5650fe2693ecf","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 3.19k/? [00:00&lt;00:00, 70.6kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7414c220f67545c6bec5db6491233ea6"}},"3cdea71db51e420388ba08b5fc139ec7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"13150a9debcb4f3f8b36c47d8b2282b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3aa1f79c730145319d4b49fede629634":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ad8ed5ec45ef4bd8867bd90267e18f48":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8ff6bd0821a44233a8b5650fe2693ecf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7414c220f67545c6bec5db6491233ea6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fdae8c1468a14b69b08a49ea35b6b0d4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_62faf21d258447278a19ae9dd870358f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_2a71d49482f049a5990dd06db901958f","IPY_MODEL_5a491e4bbccb4ea3ae1fb80835d25457","IPY_MODEL_3bb4872119c84fdda0dc01cf3072e69d"]}},"62faf21d258447278a19ae9dd870358f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2a71d49482f049a5990dd06db901958f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_367541b1a7c54497800b3f2c5a28abaa","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: ","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a2f583e16e17405e81cd940f8b7ca719"}},"5a491e4bbccb4ea3ae1fb80835d25457":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_8bc15b33b4e34b0a97301829e7cb61f2","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":2088,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2088,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a585a738f8f3454da21f463fe9acf9c8"}},"3bb4872119c84fdda0dc01cf3072e69d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_cf262f8518114c1892c6f5f31b44adb0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 5.45k/? [00:00&lt;00:00, 122kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_417e6df9bb2d44389964d5b613d4c9b4"}},"367541b1a7c54497800b3f2c5a28abaa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a2f583e16e17405e81cd940f8b7ca719":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8bc15b33b4e34b0a97301829e7cb61f2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a585a738f8f3454da21f463fe9acf9c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cf262f8518114c1892c6f5f31b44adb0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"417e6df9bb2d44389964d5b613d4c9b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5a261f95cb564cf896f7c083a80d2446":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_fef43b5ad29d48a6a6e7c14ae11b41d6","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_14fe83bea6da4590a75b65a31bfeb971","IPY_MODEL_4571e4b42bc54e6eb036d25f361d4fa6","IPY_MODEL_a7531a3c91e74adfaba45cb0d236466d"]}},"fef43b5ad29d48a6a6e7c14ae11b41d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"14fe83bea6da4590a75b65a31bfeb971":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8f67549b9fc34a6a9d1d93f24aaa3699","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: ","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_aa8d64da1ab74b14acd0efaaeea87ede"}},"4571e4b42bc54e6eb036d25f361d4fa6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_859811955444460aa2fdb0e04965a427","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":2059,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2059,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_df88ebae722844e28d0623c6b0a00639"}},"a7531a3c91e74adfaba45cb0d236466d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9f7b88b3a7ab4df7886f8b1cd4d4ae39","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 5.27k/? [00:00&lt;00:00, 112kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c9f47c1be7514609bca21b544c09551a"}},"8f67549b9fc34a6a9d1d93f24aaa3699":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"aa8d64da1ab74b14acd0efaaeea87ede":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"859811955444460aa2fdb0e04965a427":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"df88ebae722844e28d0623c6b0a00639":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9f7b88b3a7ab4df7886f8b1cd4d4ae39":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c9f47c1be7514609bca21b544c09551a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"209e12b13aaa49f5a8cd9162b3710caf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_9bc35f112aa747d7b84490f655ee7e85","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_7de855774d3f4ce299236c58138029f4","IPY_MODEL_12d9432d212f46c59b7a1d8db90deea0","IPY_MODEL_f7ba0aceb7234e60b7ff057e60d99fba"]}},"9bc35f112aa747d7b84490f655ee7e85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7de855774d3f4ce299236c58138029f4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_02c95efdae56486eba58ef69377c48ae","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: ","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cad0af0305a44c7787ffc97f7f9db600"}},"12d9432d212f46c59b7a1d8db90deea0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6c44b49c37544cd08866be40ac8074cc","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":2090,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2090,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_78a71f2a0afe4e83aae8582e73a0f6c7"}},"f7ba0aceb7234e60b7ff057e60d99fba":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_52371387d2f9467fb0a996bf616709d3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 5.38k/? [00:00&lt;00:00, 128kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a13bb166d6c04b3fa24786d2148477c2"}},"02c95efdae56486eba58ef69377c48ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"cad0af0305a44c7787ffc97f7f9db600":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6c44b49c37544cd08866be40ac8074cc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"78a71f2a0afe4e83aae8582e73a0f6c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"52371387d2f9467fb0a996bf616709d3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a13bb166d6c04b3fa24786d2148477c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6f7975c411f647aeacf7b426fac58142":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5025087223524654b6aa80626ad3f968","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_8f8f40d13e0847618c45f8bea54f4269","IPY_MODEL_e00ebfd4726c4403a0e6d475e4c5f009","IPY_MODEL_071ba2ba6e734cbc82d2ab5ffccbff7b"]}},"5025087223524654b6aa80626ad3f968":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8f8f40d13e0847618c45f8bea54f4269":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3cb368a2e2f6493993d3806d6bdec945","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Dl Completed...: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3112881168904c3184338f81fa2892a6"}},"e00ebfd4726c4403a0e6d475e4c5f009":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1f238a6b2c7e497bb24a8b70ee9b3d36","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1dc00a828bd24730b176efece4e920b4"}},"071ba2ba6e734cbc82d2ab5ffccbff7b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_dba3a2082d3944278e6710b7d54c09f6","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [01:10&lt;00:00, 62.71s/ url]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d7be4e8ede1b4f5ebc9b4421ed1d2de2"}},"3cb368a2e2f6493993d3806d6bdec945":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3112881168904c3184338f81fa2892a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1f238a6b2c7e497bb24a8b70ee9b3d36":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1dc00a828bd24730b176efece4e920b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":"20px","min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dba3a2082d3944278e6710b7d54c09f6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d7be4e8ede1b4f5ebc9b4421ed1d2de2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1b21f018353144079e3011ea7cb67ce7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3addc98a5c6b4f619bb498079e9c2f85","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_6d605433d3e148959c14ed783d18528f","IPY_MODEL_b20d8b61db6a4845b9f111cacc44f611","IPY_MODEL_445728b1e2374b1b931d48198707e901"]}},"3addc98a5c6b4f619bb498079e9c2f85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6d605433d3e148959c14ed783d18528f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7484459be1254ac381d4e16f7b639e53","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Dl Size...: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3537b60865f44b4c9070cef3731b76aa"}},"b20d8b61db6a4845b9f111cacc44f611":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7fb4169c41a346749fbfa3eecdbd907d","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e5a79b4d0aa4420cad0aef9231129ed2"}},"445728b1e2374b1b931d48198707e901":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d31aaaba8b3246808e4fdeca39142bb3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 90/90 [01:10&lt;00:00,  1.10 MiB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e1a30b16fc1b4f938ad09e4f0fbc629c"}},"7484459be1254ac381d4e16f7b639e53":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3537b60865f44b4c9070cef3731b76aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7fb4169c41a346749fbfa3eecdbd907d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e5a79b4d0aa4420cad0aef9231129ed2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":"20px","min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d31aaaba8b3246808e4fdeca39142bb3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e1a30b16fc1b4f938ad09e4f0fbc629c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3547c3eb60bf493bbe0474898231b72f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_90abe14600c24646b8ff15bf5d74cf8c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_2fa44526524a46c9a980094a2b3b3f24","IPY_MODEL_860b283216924a96bf92f05a1372b9df","IPY_MODEL_fa329f598ea64a8a96ee4ea398057629"]}},"90abe14600c24646b8ff15bf5d74cf8c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2fa44526524a46c9a980094a2b3b3f24":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_138f061db49842e18181c26fb8935399","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Extraction completed...: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b76c2954c5684f2daa843e749a8d1251"}},"860b283216924a96bf92f05a1372b9df":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b922782e1a504075a75c86372ec9b246","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3e7c46d6ce8a424499f2afd925ec4076"}},"fa329f598ea64a8a96ee4ea398057629":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_15fdbffd246642f38eb8ccd2c2895fce","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [01:10&lt;00:00, 70.03s/ file]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_97f19e5140e6466cbb94e1c27a34b866"}},"138f061db49842e18181c26fb8935399":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b76c2954c5684f2daa843e749a8d1251":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b922782e1a504075a75c86372ec9b246":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"3e7c46d6ce8a424499f2afd925ec4076":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":"20px","min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"15fdbffd246642f38eb8ccd2c2895fce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"97f19e5140e6466cbb94e1c27a34b866":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b6962cf6f47d4187a1642ebecad9e7e0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_6595341822cb4fa5b9e223faec0647cb","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_5da9e7c8522e468e935e80c388c67508","IPY_MODEL_495a6ddbc7294da1b88e97437a5ee265","IPY_MODEL_f6ca29cc1d024834a98acb781360762b"]}},"6595341822cb4fa5b9e223faec0647cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5da9e7c8522e468e935e80c388c67508":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e0a20147f1a344cdb4eb7161935102c4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_75be630f0bf74750aba4f01d3037b1e4"}},"495a6ddbc7294da1b88e97437a5ee265":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7f9b91b38b64490eb6ecc267e0b584e7","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_64492b3476de4cec9f906d9fb770adc5"}},"f6ca29cc1d024834a98acb781360762b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1ce7a3098e16417488e112f646365884","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 9983/0 [00:04&lt;00:00, 2251.60 examples/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cf9f7fb99fb54062b8f0aa5d64555ebb"}},"e0a20147f1a344cdb4eb7161935102c4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"75be630f0bf74750aba4f01d3037b1e4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7f9b91b38b64490eb6ecc267e0b584e7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"64492b3476de4cec9f906d9fb770adc5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":"20px","min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1ce7a3098e16417488e112f646365884":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"cf9f7fb99fb54062b8f0aa5d64555ebb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"313722def808401c8fe29561f1fabbf6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d93fd12a38f146c5927efdfdbd8ee65e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_6305c9f3c82d4b779f60810209c7c715","IPY_MODEL_9aa0f219237643f0a22e1fecac19f801","IPY_MODEL_3eab95eeb29840d4bd808bf3a734292e"]}},"d93fd12a38f146c5927efdfdbd8ee65e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6305c9f3c82d4b779f60810209c7c715":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5b13850993c1488f8cb92e8f46c0b3b7","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7bad7a54cd11485f8e58c84a8bd94cbb"}},"9aa0f219237643f0a22e1fecac19f801":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_0411244162264a82bad69fca45a17848","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"danger","max":10000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":9999,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_594c3da67258485590f92226ef3d807a"}},"3eab95eeb29840d4bd808bf3a734292e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7591c8cf69a84bbeab20579258a04aa6","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 9999/10000 [00:00&lt;00:00, 14158.06 examples/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b1631c1b5eb14b8ab12d2982694355db"}},"5b13850993c1488f8cb92e8f46c0b3b7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7bad7a54cd11485f8e58c84a8bd94cbb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0411244162264a82bad69fca45a17848":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"594c3da67258485590f92226ef3d807a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7591c8cf69a84bbeab20579258a04aa6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b1631c1b5eb14b8ab12d2982694355db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"73fe469f60f643d0ab7b78045bcfa787":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_35acd9b85f544b1aad2e7277493070ef","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_2b3287c540e34e1986c7094640dff594","IPY_MODEL_889067da3aca497584db6257a02d7f6b","IPY_MODEL_ac5ebb3e45be461ab6d6e785b5a6cb58"]}},"35acd9b85f544b1aad2e7277493070ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2b3287c540e34e1986c7094640dff594":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b746c925becb4953b63b53e3068b08a3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_86973af804184ae485e68b67ab4a3fbe"}},"889067da3aca497584db6257a02d7f6b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c2a12a5464b54bd6b98611f925af0be1","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_dc10aeac292545d7a8f7817eb6590ede"}},"ac5ebb3e45be461ab6d6e785b5a6cb58":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a0ed56b843d9405ba51528ddfdd0f370","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 9814/0 [00:04&lt;00:00, 2458.53 examples/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a51b3d7e6c2340bcb01453929e1724be"}},"b746c925becb4953b63b53e3068b08a3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"86973af804184ae485e68b67ab4a3fbe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c2a12a5464b54bd6b98611f925af0be1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"dc10aeac292545d7a8f7817eb6590ede":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":"20px","min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a0ed56b843d9405ba51528ddfdd0f370":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a51b3d7e6c2340bcb01453929e1724be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8b67a940eee64131a096b51090faefbe":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_bb5a9c07a4f947c98548b62d9445729f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4ca5b1a897874532a88e7f5081508d71","IPY_MODEL_79ba350719d242cbb56fd46db52ed402","IPY_MODEL_b36734d202e4483fb0334c9789f0d05b"]}},"bb5a9c07a4f947c98548b62d9445729f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4ca5b1a897874532a88e7f5081508d71":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ede00d9618d84ab18290dfcbdccac47f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_eebe93654a4741abb9b71874229005c8"}},"79ba350719d242cbb56fd46db52ed402":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_8fe3068c9d5c448bba778a536b221a6b","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"danger","max":10000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":9999,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3703d16c053e4f99a2f7d6683e570f14"}},"b36734d202e4483fb0334c9789f0d05b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b535ae3c178746629efe29fc122d0c90","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 9999/10000 [00:00&lt;00:00, 87739.62 examples/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_20e73ffbada74d868b6f9ac253034219"}},"ede00d9618d84ab18290dfcbdccac47f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"eebe93654a4741abb9b71874229005c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8fe3068c9d5c448bba778a536b221a6b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"3703d16c053e4f99a2f7d6683e570f14":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b535ae3c178746629efe29fc122d0c90":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"20e73ffbada74d868b6f9ac253034219":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0d181713f6554e1091c537854c3d5a93":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_330a8ac5390a405ca5a1aabebfc362c1","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3af2ede7f79d44b9b9470a37091de632","IPY_MODEL_f7ff3a6822334a078116bb18b1d8166f","IPY_MODEL_87ddfebb925942b383724d82009849be"]}},"330a8ac5390a405ca5a1aabebfc362c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3af2ede7f79d44b9b9470a37091de632":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3b0df5afce82490bbba652ed2f874697","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_26934fe47cd845118a896725441c4f53"}},"f7ff3a6822334a078116bb18b1d8166f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e3fa6a173642486896777a0dd20517e1","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"danger","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5ab527e41b7649ad913a31284977c74f"}},"87ddfebb925942b383724d82009849be":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_243642ad09044a66afff58dd275f6e41","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 160052/0 [01:10&lt;00:00, 2225.92 examples/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_de9db71434d14be19caca6307ee66137"}},"3b0df5afce82490bbba652ed2f874697":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"26934fe47cd845118a896725441c4f53":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e3fa6a173642486896777a0dd20517e1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"5ab527e41b7649ad913a31284977c74f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":"20px","min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"243642ad09044a66afff58dd275f6e41":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"de9db71434d14be19caca6307ee66137":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["6087f8c8c090440fb72b605abf3a9ef8","7671ac6a78e640dbbdd26a4c9e36055a","f580e65976524bfa8f8fa8e1e645bdef","45548de77288480290e7dc884582d4cc","dddc0ccf4e7f485bb874791f761ecf00","98272c68c1854032a565837a10625a57","5bcf46146ddc46498c231bf23e81f478","4d40297703934b18ac2ddda8494311b6","8340d1f90e2640b189122a0e5074e7ec","70a7a00ef4a540ac8c01804326e4d5af","f61c75d28ee74a0ebecbba0988a99702","20371a9a0dfc4252b2e6841b65941c9b","cabce981c2104b1caf745e7ac672afc2","d61045e7d2a2421abdd67e2aec16a9fd","ea0441251d85468ea4dc6806fe8bb39c","cf121efe0b294a2990bc82f0e1b7aead","3cdea71db51e420388ba08b5fc139ec7","13150a9debcb4f3f8b36c47d8b2282b1","3aa1f79c730145319d4b49fede629634","ad8ed5ec45ef4bd8867bd90267e18f48","8ff6bd0821a44233a8b5650fe2693ecf","7414c220f67545c6bec5db6491233ea6","fdae8c1468a14b69b08a49ea35b6b0d4","62faf21d258447278a19ae9dd870358f","2a71d49482f049a5990dd06db901958f","5a491e4bbccb4ea3ae1fb80835d25457","3bb4872119c84fdda0dc01cf3072e69d","367541b1a7c54497800b3f2c5a28abaa","a2f583e16e17405e81cd940f8b7ca719","8bc15b33b4e34b0a97301829e7cb61f2","a585a738f8f3454da21f463fe9acf9c8","cf262f8518114c1892c6f5f31b44adb0","417e6df9bb2d44389964d5b613d4c9b4","5a261f95cb564cf896f7c083a80d2446","fef43b5ad29d48a6a6e7c14ae11b41d6","14fe83bea6da4590a75b65a31bfeb971","4571e4b42bc54e6eb036d25f361d4fa6","a7531a3c91e74adfaba45cb0d236466d","8f67549b9fc34a6a9d1d93f24aaa3699","aa8d64da1ab74b14acd0efaaeea87ede","859811955444460aa2fdb0e04965a427","df88ebae722844e28d0623c6b0a00639","9f7b88b3a7ab4df7886f8b1cd4d4ae39","c9f47c1be7514609bca21b544c09551a","209e12b13aaa49f5a8cd9162b3710caf","9bc35f112aa747d7b84490f655ee7e85","7de855774d3f4ce299236c58138029f4","12d9432d212f46c59b7a1d8db90deea0","f7ba0aceb7234e60b7ff057e60d99fba","02c95efdae56486eba58ef69377c48ae","cad0af0305a44c7787ffc97f7f9db600","6c44b49c37544cd08866be40ac8074cc","78a71f2a0afe4e83aae8582e73a0f6c7","52371387d2f9467fb0a996bf616709d3","a13bb166d6c04b3fa24786d2148477c2","6f7975c411f647aeacf7b426fac58142","5025087223524654b6aa80626ad3f968","8f8f40d13e0847618c45f8bea54f4269","e00ebfd4726c4403a0e6d475e4c5f009","071ba2ba6e734cbc82d2ab5ffccbff7b","3cb368a2e2f6493993d3806d6bdec945","3112881168904c3184338f81fa2892a6","1f238a6b2c7e497bb24a8b70ee9b3d36","1dc00a828bd24730b176efece4e920b4","dba3a2082d3944278e6710b7d54c09f6","d7be4e8ede1b4f5ebc9b4421ed1d2de2","1b21f018353144079e3011ea7cb67ce7","3addc98a5c6b4f619bb498079e9c2f85","6d605433d3e148959c14ed783d18528f","b20d8b61db6a4845b9f111cacc44f611","445728b1e2374b1b931d48198707e901","7484459be1254ac381d4e16f7b639e53","3537b60865f44b4c9070cef3731b76aa","7fb4169c41a346749fbfa3eecdbd907d","e5a79b4d0aa4420cad0aef9231129ed2","d31aaaba8b3246808e4fdeca39142bb3","e1a30b16fc1b4f938ad09e4f0fbc629c","3547c3eb60bf493bbe0474898231b72f","90abe14600c24646b8ff15bf5d74cf8c","2fa44526524a46c9a980094a2b3b3f24","860b283216924a96bf92f05a1372b9df","fa329f598ea64a8a96ee4ea398057629","138f061db49842e18181c26fb8935399","b76c2954c5684f2daa843e749a8d1251","b922782e1a504075a75c86372ec9b246","3e7c46d6ce8a424499f2afd925ec4076","15fdbffd246642f38eb8ccd2c2895fce","97f19e5140e6466cbb94e1c27a34b866","b6962cf6f47d4187a1642ebecad9e7e0","6595341822cb4fa5b9e223faec0647cb","5da9e7c8522e468e935e80c388c67508","495a6ddbc7294da1b88e97437a5ee265","f6ca29cc1d024834a98acb781360762b","e0a20147f1a344cdb4eb7161935102c4","75be630f0bf74750aba4f01d3037b1e4","7f9b91b38b64490eb6ecc267e0b584e7","64492b3476de4cec9f906d9fb770adc5","1ce7a3098e16417488e112f646365884","cf9f7fb99fb54062b8f0aa5d64555ebb","313722def808401c8fe29561f1fabbf6","d93fd12a38f146c5927efdfdbd8ee65e","6305c9f3c82d4b779f60810209c7c715","9aa0f219237643f0a22e1fecac19f801","3eab95eeb29840d4bd808bf3a734292e","5b13850993c1488f8cb92e8f46c0b3b7","7bad7a54cd11485f8e58c84a8bd94cbb","0411244162264a82bad69fca45a17848","594c3da67258485590f92226ef3d807a","7591c8cf69a84bbeab20579258a04aa6","b1631c1b5eb14b8ab12d2982694355db","73fe469f60f643d0ab7b78045bcfa787","35acd9b85f544b1aad2e7277493070ef","2b3287c540e34e1986c7094640dff594","889067da3aca497584db6257a02d7f6b","ac5ebb3e45be461ab6d6e785b5a6cb58","b746c925becb4953b63b53e3068b08a3","86973af804184ae485e68b67ab4a3fbe","c2a12a5464b54bd6b98611f925af0be1","dc10aeac292545d7a8f7817eb6590ede","a0ed56b843d9405ba51528ddfdd0f370","a51b3d7e6c2340bcb01453929e1724be","8b67a940eee64131a096b51090faefbe","bb5a9c07a4f947c98548b62d9445729f","4ca5b1a897874532a88e7f5081508d71","79ba350719d242cbb56fd46db52ed402","b36734d202e4483fb0334c9789f0d05b","ede00d9618d84ab18290dfcbdccac47f","eebe93654a4741abb9b71874229005c8","8fe3068c9d5c448bba778a536b221a6b","3703d16c053e4f99a2f7d6683e570f14","b535ae3c178746629efe29fc122d0c90","20e73ffbada74d868b6f9ac253034219","0d181713f6554e1091c537854c3d5a93","330a8ac5390a405ca5a1aabebfc362c1","3af2ede7f79d44b9b9470a37091de632","f7ff3a6822334a078116bb18b1d8166f","87ddfebb925942b383724d82009849be","3b0df5afce82490bbba652ed2f874697","26934fe47cd845118a896725441c4f53","e3fa6a173642486896777a0dd20517e1","5ab527e41b7649ad913a31284977c74f","243642ad09044a66afff58dd275f6e41","de9db71434d14be19caca6307ee66137"]},"id":"Hz1yc3GZbMp3","executionInfo":{"status":"error","timestamp":1646288875907,"user_tz":-330,"elapsed":357615,"user":{"displayName":"Anunay Yadav","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhwwpkT-fB7xFD2pOYIajY3v4O-5ZcrswGNcQBQWA=s64","userId":"13986630515992640352"}},"outputId":"3af03d56-f3b6-40c7-fa7a-4f25fecaf0d5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.16.2)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Collecting seqeval\n","  Using cached seqeval-1.2.2.tar.gz (43 kB)\n","Collecting datasets\n","  Using cached datasets-1.18.3-py3-none-any.whl (311 kB)\n","Collecting allennlp\n","  Using cached allennlp-2.9.0-py3-none-any.whl (716 kB)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.21.5)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.0.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n","Collecting fsspec[http]>=2021.05.0\n","  Using cached fsspec-2022.2.0-py3-none-any.whl (134 kB)\n","Collecting aiohttp\n","  Using cached aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Collecting xxhash\n","  Using cached xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.4.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n","Collecting jsonnet>=0.10.0\n","  Using cached jsonnet-0.18.0.tar.gz (592 kB)\n","Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.99)\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from allennlp) (8.12.0)\n","Requirement already satisfied: torch<1.11.0,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.10.0+cu111)\n","Requirement already satisfied: nltk<3.6.6 in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.2.5)\n","Collecting base58\n","  Using cached base58-2.1.1-py3-none-any.whl (5.6 kB)\n","Collecting cached-path<2.0.0,>=1.0.2\n","  Using cached cached_path-1.0.2-py3-none-any.whl (26 kB)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.1.96)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.6.4)\n","Collecting fairscale==0.4.5\n","  Using cached fairscale-0.4.5.tar.gz (240 kB)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: spacy<3.3,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.2.4)\n","Collecting filelock\n","  Using cached filelock-3.4.2-py3-none-any.whl (9.9 kB)\n","Requirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.1.0)\n","Collecting wandb<0.13.0,>=0.10.0\n","  Using cached wandb-0.12.11-py2.py3-none-any.whl (1.7 MB)\n","Requirement already satisfied: torchvision<0.12.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.11.1+cu111)\n","Collecting checklist==0.0.11\n","  Using cached checklist-0.0.11.tar.gz (12.1 MB)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.1.0)\n","Collecting transformers<4.16,>=4.1\n","  Using cached transformers-4.15.0-py3-none-any.whl (3.4 MB)\n","Collecting tensorboardX>=1.2\n","  Using cached tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\n","Collecting munch>=2.5\n","  Using cached munch-2.5.0-py2.py3-none-any.whl (10 kB)\n","Requirement already satisfied: jupyter>=1.0 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp) (1.0.0)\n","Requirement already satisfied: ipywidgets>=7.5 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp) (7.6.5)\n","Collecting patternfork-nosql\n","  Using cached patternfork_nosql-3.6.tar.gz (22.3 MB)\n","Collecting iso-639\n","  Using cached iso-639-0.4.5.tar.gz (167 kB)\n","Collecting huggingface-hub<1.0.0,>=0.1.0\n","  Using cached huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n","Collecting boto3<2.0,>=1.0\n","  Using cached boto3-1.21.11-py3-none-any.whl (132 kB)\n","Requirement already satisfied: google-cloud-storage<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from cached-path<2.0.0,>=1.0.2->allennlp) (1.18.1)\n","Collecting jmespath<1.0.0,>=0.7.1\n","  Using cached jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n","Collecting botocore<1.25.0,>=1.24.11\n","  Using cached botocore-1.24.11-py3-none-any.whl (8.6 MB)\n","Collecting s3transfer<0.6.0,>=0.5.0\n","  Using cached s3transfer-0.5.2-py3-none-any.whl (79 kB)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.25.0,>=1.24.11->boto3<2.0,>=1.0->cached-path<2.0.0,>=1.0.2->allennlp) (2.8.2)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<2.0,>=1.0->cached-path<2.0.0,>=1.0.2->allennlp) (1.0.3)\n","Requirement already satisfied: google-resumable-media<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<2.0,>=1.0->cached-path<2.0.0,>=1.0.2->allennlp) (0.4.1)\n","Requirement already satisfied: google-auth>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<2.0,>=1.0->cached-path<2.0.0,>=1.0.2->allennlp) (1.35.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2.0->google-cloud-storage<2.0,>=1.0->cached-path<2.0.0,>=1.0.2->allennlp) (1.15.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2.0->google-cloud-storage<2.0,>=1.0->cached-path<2.0.0,>=1.0.2->allennlp) (4.8)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2.0->google-cloud-storage<2.0,>=1.0->cached-path<2.0.0,>=1.0.2->allennlp) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2.0->google-cloud-storage<2.0,>=1.0->cached-path<2.0.0,>=1.0.2->allennlp) (4.2.4)\n","Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2.0->google-cloud-storage<2.0,>=1.0->cached-path<2.0.0,>=1.0.2->allennlp) (57.4.0)\n","Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage<2.0,>=1.0->cached-path<2.0.0,>=1.0.2->allennlp) (1.26.3)\n","Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage<2.0,>=1.0->cached-path<2.0.0,>=1.0.2->allennlp) (1.55.0)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage<2.0,>=1.0->cached-path<2.0.0,>=1.0.2->allennlp) (2018.9)\n","Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage<2.0,>=1.0->cached-path<2.0.0,>=1.0.2->allennlp) (3.17.3)\n","Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (4.10.1)\n","Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.1.1)\n","Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.2.0)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (1.0.2)\n","Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (3.5.2)\n","Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.1.3)\n","Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.5.0)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.3.5)\n","Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.1.1)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (4.8.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (4.4.2)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.8.1)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.7.5)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (1.0.18)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (2.6.1)\n","Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp) (5.2.0)\n","Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp) (5.2.2)\n","Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp) (5.3.1)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp) (5.6.1)\n","Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (4.9.2)\n","Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (4.3.3)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.4.0)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (21.4.0)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.18.1)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (3.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.2.5)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2.0->google-cloud-storage<2.0,>=1.0->cached-path<2.0.0,>=1.0.2->allennlp) (0.4.8)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3,>=2.1.0->allennlp) (2.0.6)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3,>=2.1.0->allennlp) (1.0.6)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3,>=2.1.0->allennlp) (1.0.0)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3,>=2.1.0->allennlp) (0.4.1)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3,>=2.1.0->allennlp) (1.0.5)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3,>=2.1.0->allennlp) (0.9.0)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3,>=2.1.0->allennlp) (7.4.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3,>=2.1.0->allennlp) (3.0.6)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3,>=2.1.0->allennlp) (1.1.3)\n","Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision<0.12.0,>=0.8.1->allennlp) (7.1.2)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<4.16,>=4.1->allennlp) (0.0.47)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Using cached tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.16,>=4.1->allennlp) (2019.12.20)\n","Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (2.3)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (5.4.8)\n","Collecting setproctitle\n","  Using cached setproctitle-1.2.2-cp37-cp37m-manylinux1_x86_64.whl (36 kB)\n","Collecting yaspin>=1.0.0\n","  Using cached yaspin-2.1.0-py3-none-any.whl (18 kB)\n","Collecting shortuuid>=0.5.0\n","  Using cached shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n","Collecting GitPython>=1.0.0\n","  Using cached GitPython-3.1.27-py3-none-any.whl (181 kB)\n","Collecting docker-pycreds>=0.4.0\n","  Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting pathtools\n","  Using cached pathtools-0.1.2.tar.gz (11 kB)\n","Collecting sentry-sdk>=1.0.0\n","  Using cached sentry_sdk-1.5.6-py2.py3-none-any.whl (144 kB)\n","Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (7.1.2)\n","Collecting gitdb<5,>=4.0.1\n","  Using cached gitdb-4.0.9-py3-none-any.whl (63 kB)\n","Collecting smmap<6,>=3.0.1\n","  Using cached smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (0.13.1)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (1.8.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (2.11.3)\n","Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp) (22.3.0)\n","Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (0.7.0)\n","Collecting asynctest==0.13.0\n","  Using cached asynctest-0.13.0-py3-none-any.whl (26 kB)\n","Collecting frozenlist>=1.1.1\n","  Using cached frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n","Collecting yarl<2.0,>=1.0\n","  Using cached yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting multidict<7.0,>=4.5\n","  Using cached multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n","Collecting aiosignal>=1.1.2\n","  Using cached aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->allennlp) (1.5.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (2.0.1)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.7.1)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (1.5.0)\n","Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.6.0)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.4)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.8.4)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (4.1.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.5.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp) (0.16.0)\n","Collecting backports.csv\n","  Using cached backports.csv-1.0.7-py2.py3-none-any.whl (12 kB)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp) (4.6.3)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp) (4.2.6)\n","Collecting feedparser\n","  Using cached feedparser-6.0.8-py3-none-any.whl (81 kB)\n","Collecting pdfminer.six\n","  Downloading pdfminer.six-20211012-py3-none-any.whl (5.6 MB)\n","\u001b[K     |████████████████████████████████| 5.6 MB 5.4 MB/s \n","\u001b[?25hCollecting python-docx\n","  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n","\u001b[K     |████████████████████████████████| 5.6 MB 39.1 MB/s \n","\u001b[?25hCollecting cherrypy\n","  Downloading CherryPy-18.6.1-py2.py3-none-any.whl (419 kB)\n","\u001b[K     |████████████████████████████████| 419 kB 45.4 MB/s \n","\u001b[?25hCollecting portend>=2.1.1\n","  Downloading portend-3.1.0-py3-none-any.whl (5.3 kB)\n","Collecting cheroot>=8.2.1\n","  Downloading cheroot-8.6.0-py2.py3-none-any.whl (104 kB)\n","\u001b[K     |████████████████████████████████| 104 kB 50.2 MB/s \n","\u001b[?25hCollecting zc.lockfile\n","  Downloading zc.lockfile-2.0-py2.py3-none-any.whl (9.7 kB)\n","Collecting jaraco.collections\n","  Downloading jaraco.collections-3.5.1-py3-none-any.whl (10 kB)\n","Collecting jaraco.functools\n","  Downloading jaraco.functools-3.5.0-py3-none-any.whl (7.0 kB)\n","Collecting tempora>=1.8\n","  Downloading tempora-5.0.1-py3-none-any.whl (15 kB)\n","Collecting sgmllib3k\n","  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n","Collecting jaraco.classes\n","  Downloading jaraco.classes-3.2.1-py3-none-any.whl (5.6 kB)\n","Collecting jaraco.text\n","  Downloading jaraco.text-3.7.0-py3-none-any.whl (8.6 kB)\n","Collecting jaraco.context>=4.1\n","  Downloading jaraco.context-4.1.1-py3-none-any.whl (4.4 kB)\n","Collecting cryptography\n","  Downloading cryptography-36.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n","\u001b[K     |████████████████████████████████| 3.6 MB 38.2 MB/s \n","\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp) (1.15.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp) (2.21)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (0.7.1)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (1.4.0)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (1.11.0)\n","Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter>=1.0->checklist==0.0.11->allennlp) (2.0.1)\n","Building wheels for collected packages: seqeval, checklist, fairscale, jsonnet, iso-639, pathtools, patternfork-nosql, python-docx, sgmllib3k\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=8655517e79b753be2168617b2487ae8bea68275ab8f116190c05f4c74822e885\n","  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n","  Building wheel for checklist (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for checklist: filename=checklist-0.0.11-py3-none-any.whl size=12165635 sha256=b6c18ef242e06e224ebcca7d8f3fd4e8f360cf23591fc8f68d2e9b7bf886844b\n","  Stored in directory: /root/.cache/pip/wheels/6a/8a/07/6446879be434879c27671c83443727d74cecf6b630c8a24d03\n","  Building wheel for fairscale (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fairscale: filename=fairscale-0.4.5-py3-none-any.whl size=297992 sha256=65e685b2461c00f87b19ce72836b1ce3027bbee3bd5eebc5057ea59b04ceb632\n","  Stored in directory: /root/.cache/pip/wheels/83/d0/90/bcfc419ab267ea41fda54216f0c99e3faf9bab9b38ec760c46\n","  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for jsonnet: filename=jsonnet-0.18.0-cp37-cp37m-linux_x86_64.whl size=3994414 sha256=c9c06ff8e7d517b245dbe598e2165ab1476d28ac413834bbcd3d8cc86675c8b3\n","  Stored in directory: /root/.cache/pip/wheels/a9/63/f9/a653f9c21575e6ff271ee6a49939aa002005174cea6c35919d\n","  Building wheel for iso-639 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for iso-639: filename=iso_639-0.4.5-py3-none-any.whl size=169061 sha256=3a0977e8682efc1a04d33040c71d3721f4482ee4ff5823b7d398aacfeb2ae61f\n","  Stored in directory: /root/.cache/pip/wheels/47/60/19/6d020fc92138ed1b113a18271e83ea4b5525fe770cb45b9a2e\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=281c842755a030415de35dc9bf4481588c4cd72e1c817ccda43d61f0bd683233\n","  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n","  Building wheel for patternfork-nosql (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for patternfork-nosql: filename=patternfork_nosql-3.6-py3-none-any.whl size=22332804 sha256=721507c9dbd37996621b874209d5f568c9403d37b1746a2c74074564c6814026\n","  Stored in directory: /root/.cache/pip/wheels/97/72/8f/5305fe28168f93b658da9ed433b9a1d3ec90594faa0c9aaf4b\n","  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184507 sha256=ef08597a1ede59bb70624b253dc900bce3000ddd227171c6593bb693656fc2c4\n","  Stored in directory: /root/.cache/pip/wheels/f6/6f/b9/d798122a8b55b74ad30b5f52b01482169b445fbb84a11797a6\n","  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6066 sha256=c2e68bed817271f305f83d7a4b1bdd9bea5ed12f517b784cec8ac3683574899b\n","  Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\n","Successfully built seqeval checklist fairscale jsonnet iso-639 pathtools patternfork-nosql python-docx sgmllib3k\n","Installing collected packages: urllib3, jaraco.functools, jaraco.context, tempora, jmespath, jaraco.text, jaraco.classes, zc.lockfile, smmap, sgmllib3k, portend, multidict, jaraco.collections, frozenlist, filelock, cryptography, cheroot, botocore, yarl, tokenizers, s3transfer, python-docx, pdfminer.six, huggingface-hub, gitdb, feedparser, cherrypy, backports.csv, asynctest, async-timeout, aiosignal, yaspin, transformers, shortuuid, setproctitle, sentry-sdk, patternfork-nosql, pathtools, munch, iso-639, GitPython, fsspec, docker-pycreds, boto3, aiohttp, xxhash, wandb, tensorboardX, jsonnet, fairscale, checklist, cached-path, base58, seqeval, datasets, allennlp\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","  Attempting uninstall: filelock\n","    Found existing installation: filelock 3.6.0\n","    Uninstalling filelock-3.6.0:\n","      Successfully uninstalled filelock-3.6.0\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.11.6\n","    Uninstalling tokenizers-0.11.6:\n","      Successfully uninstalled tokenizers-0.11.6\n","  Attempting uninstall: huggingface-hub\n","    Found existing installation: huggingface-hub 0.4.0\n","    Uninstalling huggingface-hub-0.4.0:\n","      Successfully uninstalled huggingface-hub-0.4.0\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.16.2\n","    Uninstalling transformers-4.16.2:\n","      Successfully uninstalled transformers-4.16.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed GitPython-3.1.27 aiohttp-3.8.1 aiosignal-1.2.0 allennlp-2.9.0 async-timeout-4.0.2 asynctest-0.13.0 backports.csv-1.0.7 base58-2.1.1 boto3-1.21.11 botocore-1.24.11 cached-path-1.0.2 checklist-0.0.11 cheroot-8.6.0 cherrypy-18.6.1 cryptography-36.0.1 datasets-1.18.3 docker-pycreds-0.4.0 fairscale-0.4.5 feedparser-6.0.8 filelock-3.4.2 frozenlist-1.3.0 fsspec-2022.2.0 gitdb-4.0.9 huggingface-hub-0.2.1 iso-639-0.4.5 jaraco.classes-3.2.1 jaraco.collections-3.5.1 jaraco.context-4.1.1 jaraco.functools-3.5.0 jaraco.text-3.7.0 jmespath-0.10.0 jsonnet-0.18.0 multidict-6.0.2 munch-2.5.0 pathtools-0.1.2 patternfork-nosql-3.6 pdfminer.six-20211012 portend-3.1.0 python-docx-0.8.11 s3transfer-0.5.2 sentry-sdk-1.5.6 seqeval-1.2.2 setproctitle-1.2.2 sgmllib3k-1.0.0 shortuuid-1.0.8 smmap-5.0.0 tempora-5.0.1 tensorboardX-2.5 tokenizers-0.10.3 transformers-4.15.0 urllib3-1.25.11 wandb-0.12.11 xxhash-3.0.0 yarl-1.7.2 yaspin-2.1.0 zc.lockfile-2.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["filelock","huggingface_hub","tokenizers","transformers","urllib3"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: flax in /usr/local/lib/python3.7/dist-packages (0.4.0)\n","Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from flax) (1.21.5)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from flax) (3.2.2)\n","Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from flax) (1.0.3)\n","Requirement already satisfied: optax in /usr/local/lib/python3.7/dist-packages (from flax) (0.1.1)\n","Requirement already satisfied: jax>=0.2.21 in /usr/local/lib/python3.7/dist-packages (from flax) (0.3.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.21->flax) (3.10.0.2)\n","Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.21->flax) (3.3.0)\n","Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.21->flax) (1.4.1)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.21->flax) (1.0.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py->jax>=0.2.21->flax) (1.15.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (2.8.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (3.0.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (0.11.0)\n","Requirement already satisfied: chex>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from optax->flax) (0.1.1)\n","Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.7/dist-packages (from optax->flax) (0.3.0+cuda11.cudnn805)\n","Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax->flax) (0.1.6)\n","Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax->flax) (0.11.2)\n","Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from jaxlib>=0.1.37->optax->flax) (2.0)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.5)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n","True\n","3\n"]},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6087f8c8c090440fb72b605abf3a9ef8","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/570M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["768\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"20371a9a0dfc4252b2e6841b65941c9b","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/1.41k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fdae8c1468a14b69b08a49ea35b6b0d4","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/2.09k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5a261f95cb564cf896f7c083a80d2446","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/2.06k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"209e12b13aaa49f5a8cd9162b3710caf","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/2.09k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\u001b[1mDownloading and preparing dataset snli/1.1.0 (download: 90.17 MiB, generated: 87.00 MiB, total: 177.17 MiB) to /root/tensorflow_datasets/snli/1.1.0...\u001b[0m\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6f7975c411f647aeacf7b426fac58142","version_minor":0,"version_major":2},"text/plain":["Dl Completed...: 0 url [00:00, ? url/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1b21f018353144079e3011ea7cb67ce7","version_minor":0,"version_major":2},"text/plain":["Dl Size...: 0 MiB [00:00, ? MiB/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3547c3eb60bf493bbe0474898231b72f","version_minor":0,"version_major":2},"text/plain":["Extraction completed...: 0 file [00:00, ? file/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","\n","\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b6962cf6f47d4187a1642ebecad9e7e0","version_minor":0,"version_major":2},"text/plain":["0 examples [00:00, ? examples/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Shuffling and writing examples to /root/tensorflow_datasets/snli/1.1.0.incompleteZUREVL/snli-test.tfrecord\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"313722def808401c8fe29561f1fabbf6","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/10000 [00:00<?, ? examples/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"73fe469f60f643d0ab7b78045bcfa787","version_minor":0,"version_major":2},"text/plain":["0 examples [00:00, ? examples/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Shuffling and writing examples to /root/tensorflow_datasets/snli/1.1.0.incompleteZUREVL/snli-validation.tfrecord\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8b67a940eee64131a096b51090faefbe","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/10000 [00:00<?, ? examples/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0d181713f6554e1091c537854c3d5a93","version_minor":0,"version_major":2},"text/plain":["0 examples [00:00, ? examples/s]"]},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-ecaff9b08c54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric7\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"macro\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_dataset_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset_label\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0mtransformer_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_token_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-ecaff9b08c54>\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(tokenizer)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Snli\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[1;32m    342\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0mdownload_and_prepare_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_and_prepare_kwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m     \u001b[0mdbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_and_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdownload_and_prepare_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mas_dataset_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36mdownload_and_prepare\u001b[0;34m(self, download_dir, download_config)\u001b[0m\n\u001b[1;32m    385\u001b[0m           self._download_and_prepare(\n\u001b[1;32m    386\u001b[0m               \u001b[0mdl_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m               download_config=download_config)\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m           \u001b[0;31m# NOTE: If modifying the lines below to put additional information in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[0;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     super(GeneratorBasedBuilder, self)._download_and_prepare(\n\u001b[1;32m   1023\u001b[0m         \u001b[0mdl_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \u001b[0mmax_examples_per_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_examples_per_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m     )\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[0;34m(self, dl_manager, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m       \u001b[0;31m# Prepare split will record examples associated to the split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mprepare_split_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m     \u001b[0;31m# Update the info object with the splits.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36m_prepare_split\u001b[0;34m(self, split_generator, max_examples_per_split)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                      hash_salt=split_generator.name)\n\u001b[1;32m   1038\u001b[0m     for key, record in utils.tqdm(generator, unit=\" examples\",\n\u001b[0;32m-> 1039\u001b[0;31m                                   total=split_info.num_examples, leave=False):\n\u001b[0m\u001b[1;32m   1040\u001b[0m       \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/text/snli.py\u001b[0m in \u001b[0;36m_generate_examples\u001b[0;34m(self, filepath)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m       \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquoting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQUOTE_NONE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gold_label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'-'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gold_label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         yield idx, {\n","\u001b[0;32m/usr/lib/python3.7/csv.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;31m# Used only for its side effect.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfieldnames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline_num\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_buf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0;34mr\"\"\"Reads the next line, keeping \\n. At EOF, returns ''.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preread_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# -*- coding: utf-8 -*-\n","\"\"\"Tensorflow_SNLI_longformer-l2norm-WindowSize.ipynb\n","\n","Automatically generated by Colaboratory.\n","\n","Original file is located at\n","    https://colab.research.google.com/drive/1cFanFD0wTlmAu2HL2i75KJ7MGbh9J6jF\n","\"\"\"\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Commented out IPython magic to ensure Python compatibility.\n","# %cd /content/drive/MyDrive/Bert_Lime/SNLI/\n","\n","!pip install transformers\n","!pip install seqeval datasets allennlp\n","!pip install flax\n","!pip install sentencepiece\n","!pip install nltk\n","!pip install gensim\n","\n","import tensorflow_datasets as tfds\n","from tensorflow_datasets.text import Snli\n","import os\n","\n","from __future__ import division\n","\n","import torch\n","import torch.nn as nn\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","class Sparsemax(nn.Module):\n","    \"\"\"Sparsemax function.\"\"\"\n","\n","    def __init__(self, dim=None):\n","        \"\"\"Initialize sparsemax activation\n","        \n","        Args:\n","            dim (int, optional): The dimension over which to apply the sparsemax function.\n","        \"\"\"\n","        super(Sparsemax, self).__init__()\n","\n","        self.dim = -1 if dim is None else dim\n","\n","    def forward(self, input):\n","        \"\"\"Forward function.\n","        Args:\n","            input (torch.Tensor): Input tensor. First dimension should be the batch size\n","        Returns:\n","            torch.Tensor: [batch_size x number_of_logits] Output tensor\n","        \"\"\"\n","        # Sparsemax currently only handles 2-dim tensors,\n","        # so we reshape to a convenient shape and reshape back after sparsemax\n","        input = input.transpose(0, self.dim)\n","        original_size = input.size()\n","        input = input.reshape(input.size(0), -1)\n","        input = input.transpose(0, 1)\n","        dim = 1\n","\n","        number_of_logits = input.size(dim)\n","\n","        # Translate input by max for numerical stability\n","        input = input - torch.max(input, dim=dim, keepdim=True)[0].expand_as(input)\n","\n","        # Sort input in descending order.\n","        # (NOTE: Can be replaced with linear time selection method described here:\n","        # http://stanford.edu/~jduchi/projects/DuchiShSiCh08.html)\n","        zs = torch.sort(input=input, dim=dim, descending=True)[0]\n","        range = torch.arange(start=1, end=number_of_logits + 1, step=1, device=device, dtype=input.dtype).view(1, -1)\n","        range = range.expand_as(zs)\n","\n","        # Determine sparsity of projection\n","        bound = 1 + range * zs\n","        cumulative_sum_zs = torch.cumsum(zs, dim)\n","        is_gt = torch.gt(bound, cumulative_sum_zs).type(input.type())\n","        k = torch.max(is_gt * range, dim, keepdim=True)[0]\n","\n","        # Compute threshold function\n","        zs_sparse = is_gt * zs\n","\n","        # Compute taus\n","        taus = (torch.sum(zs_sparse, dim, keepdim=True) - 1) / k\n","        taus = taus.expand_as(input)\n","\n","        # Sparsemax\n","        self.output = torch.max(torch.zeros_like(input), input - taus)\n","\n","        # Reshape back to original shape\n","        output = self.output\n","        output = output.transpose(0, 1)\n","        output = output.reshape(original_size)\n","        output = output.transpose(0, self.dim)\n","\n","        return output\n","\n","    def backward(self, grad_output):\n","        \"\"\"Backward function.\"\"\"\n","        dim = 1\n","\n","        nonzeros = torch.ne(self.output, 0)\n","        sum = torch.sum(grad_output * nonzeros, dim=dim) / torch.sum(nonzeros, dim=dim)\n","        self.grad_input = nonzeros * (grad_output - sum.expand_as(grad_output))\n","\n","        return self.grad_input\n","\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n","\n","ac_dict = {\"neutral\" : 1, \"entailment\" : 0, \"contradiction\" : 2}\n","\n","import os\n","from operator import itemgetter    \n","import numpy as np\n","# import pandas as pd\n","import matplotlib.pyplot as plt\n","import warnings\n","warnings.filterwarnings('ignore')\n","get_ipython().magic(u'matplotlib inline')\n","plt.style.use('ggplot')\n","\n","# from keras import models, regularizers, layers, optimizers, losses, metrics\n","# from keras.models import Sequential\n","# from keras.layers import Dense\n","# from tensorflow.keras.utils import to_categorical\n","# from keras.layers import LayerNormalization\n","import tensorflow as tf\n","from tensorflow.keras.datasets import imdb\n","\n","import torch\n","\n","print(torch.cuda.is_available())\n","\n","print(len(ac_dict))\n","\n","model_version = 'allenai/longformer-base-4096'\n","import torch\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","import torch.nn as nn\n","import torch\n","\n","\n","for window_size in [32, 64, 128, 256, 512]:\n","    from huggingface_hub import create_repo\n","\n","    from transformers import LongformerTokenizer, LongformerModel\n","\n","    from transformers import LongformerConfig, LongformerTokenizer, LongformerModel\n","\n","    tokenizer = LongformerTokenizer.from_pretrained(model_version,\n","                                            bos_token = \"[CLS]\",\n","                                            eos_token = \"[SEP]\")\n","    transformer_model = LongformerModel.from_pretrained(model_version, attention_window = 64, output_attentions = True).to(device)\n","    linear_layer = nn.Linear(transformer_model.config.hidden_size,\n","                            len(ac_dict)).to(device)\n","    cross_entropy_layer = nn.CrossEntropyLoss()\n","    normalizing_layer = Sparsemax(dim = 1)\n","\n","    # # del transformer_model\n","    # del optimizer\n","\n","    print(transformer_model.config.hidden_size)\n","\n","    device\n","\n","    from tqdm import tqdm\n","\n","    from typing import List, Tuple\n","\n","    def pad_batch(elems: List[List[int]], pad_token_id: int) -> List[List[int]]:\n","        \"\"\"Pads all lists in elems to the maximum list length of any list in \n","        elems. Pads with pad_token_id.\n","        \"\"\"\n","        max_len = max([len(elem) for elem in elems])\n","        return [elem+[pad_token_id]*(max_len-len(elem)) for elem in elems]\n","\n","    def load_dataset(tokenizer):\n","        train, test = tfds.as_numpy(tfds.load(\"Snli\", split=['train', 'test']))\n","\n","        parts = []\n","\n","        new_train_data = []\n","        train_labels = []\n","        cnt = 0\n","        for tokenized_text in tqdm(train):\n","            cnt += 1\n","            if(cnt >= 10000):\n","                break\n","            decoded_sent = \"[CLS] \" +  str(tokenized_text[\"hypothesis\"])[2:-1] + \" [SEP] \" + str(tokenized_text[\"premise\"])[2:-1] + \" [SEP]\"\n","            # print(decoded_sent)\n","            new_encoded_text = tokenizer.encode(decoded_sent)\n","            if(tokenized_text[\"label\"] < 0 or tokenized_text[\"label\"] > 2):\n","                continue\n","            new_train_data.append(new_encoded_text)\n","            train_labels.append(tokenized_text[\"label\"])\n","        \n","        new_test_data = []\n","        test_labels = []\n","        for tokenized_text in tqdm(test):\n","            decoded_sent = \"[CLS] \" +  str(tokenized_text[\"hypothesis\"])[2:-1] + \" [SEP] \" + str(tokenized_text[\"premise\"])[2:-1] + \" [SEP]\"\n","            # print(decoded_sent)\n","            new_encoded_text = tokenizer.encode(decoded_sent)\n","            if(tokenized_text[\"label\"] < 0 or tokenized_text[\"label\"] > 2):\n","                continue\n","            new_test_data.append(new_encoded_text)\n","            test_labels.append(tokenized_text[\"label\"])\n","\n","\n","        return (new_train_data, train_labels), (new_test_data, test_labels)\n","\n","    def generator(dataset_data, dataset_label, max_len = 512, batch_size = 16):\n","        i = 0\n","        tokenized_threads, labels = [], []\n","        while i<len(dataset_data):\n","            tokenized_threads.append(dataset_data[i][:max_len])\n","            labels.append(dataset_label[i])\n","            i += 1\n","                \n","            if i%batch_size==0:\n","                yield (pad_batch(tokenized_threads, tokenizer.pad_token_id), \n","                            labels)\n","                tokenized_threads, labels = [], []\n","\n","    from itertools import chain\n","\n","    import torch.optim as optim\n","\n","    optimizer = optim.Adam(params = chain(transformer_model.parameters(),\n","                                        linear_layer.parameters()),\n","                        lr = 2e-5)\n","\n","\n","\n","\n","\n","\n","\n","    def train(dataset_data, dataset_label, batch_size):\n","        global values_weight, values_bias;\n","        accumulate_over = 32\n","        \n","        optimizer.zero_grad()\n","        print(\"Training\")\n","        for i, (tokenized_threads, labels) in enumerate(generator(dataset_data, dataset_label,  batch_size = batch_size)):\n","            \n","            #Cast to PyTorch tensor\n","            tokenized_threads = torch.tensor(tokenized_threads, device=device)\n","            labels = torch.tensor(labels, device=device, dtype=torch.long)\n","\n","\n","            loss = compute((tokenized_threads, \n","                            labels,), False)\n","\n","            print(\"\\rLoss: \", loss.item(), end = \" \")\n","\n","            loss.backward()\n","\n","            if i%accumulate_over==accumulate_over-1:\n","                optimizer.step()\n","                optimizer.zero_grad()\n","        print()\n","        optimizer.step()\n","\n","    from datasets import load_metric\n","    metric1 = load_metric(\"accuracy\")\n","    metric3 = load_metric(\"precision\", average = \"micro\")\n","    metric2 = load_metric(\"f1\", average = \"micro\")\n","    metric4 = load_metric(\"recall\", average = \"micro\")\n","    metric5 = load_metric(\"precision\", average = \"macro\")\n","    metric6 = load_metric(\"f1\", average = \"macro\")\n","    metric7 = load_metric(\"recall\", average = \"macro\")\n","\n","    def evaluate(dataset_data, dataset_label, BATCH_SIZE):\n","        \n","        int_to_labels = {v:k for k, v in ac_dict.items()}\n","        print('Evaluation')\n","        \n","        with torch.no_grad():\n","            for i, (tokenized_threads, labels) in enumerate(generator(dataset_data, dataset_label, batch_size = BATCH_SIZE)):\n","                # print(comp_type_labels)\n","                #Cast to PyTorch tensor\n","                tokenized_threads = torch.tensor(tokenized_threads, device=device)\n","                labels = torch.tensor(labels, device=device)\n","\n","                preds = compute((tokenized_threads, \n","                                labels,), pred=True)\n","                loss = compute((tokenized_threads, labels,), pred = False)\n","                \n","                metric1.add_batch(predictions=preds, \n","                                references=labels,)\n","                                #tokenized_threads=tokenized_threads.cpu().tolist())\n","                metric2.add_batch(predictions=preds, \n","                                references=labels,)\n","                metric3.add_batch(predictions=preds, \n","                                references=labels,)\n","                metric4.add_batch(predictions=preds, \n","                                references=labels,)\n","                metric5.add_batch(predictions=preds, \n","                                references=labels,)\n","                metric6.add_batch(predictions=preds, \n","                                references=labels,)\n","                metric7.add_batch(predictions=preds, \n","                                references=labels,)\n","                \n","                print(\"\\rLoss: \", loss.item(), end = \" \")\n","        print()\n","        print(metric1.compute())\n","        print(\"Micro\")\n","        print(metric2.compute(average = \"micro\"))\n","        print(metric3.compute(average = \"micro\"))\n","        print(metric4.compute(average = \"micro\"))\n","        print(\"Macro\")\n","        print(metric5.compute(average = \"macro\"))\n","        print(metric6.compute(average = \"macro\"))\n","        print(metric7.compute(average = \"macro\"))\n","\n","    (train_dataset_data, train_dataset_label), (test_dataset_data, test_dataset_label) = load_dataset(tokenizer)\n","\n","    transformer_model.resize_token_embeddings(len(tokenizer))\n","\n","    def compute(batch: Tuple[torch.Tensor, torch.Tensor], pred: bool=True):\n","        \"\"\"\n","        Args:\n","            batch:  A tuple having tokenized thread of shape [batch_size, seq_len],\n","                    component type labels of shape [batch_size, seq_len], and a global\n","                    attention mask for Longformer, of the same shape.\n","            \n","            \n","            cross_entropy:  This argument will only be used if preds=False, i.e., if \n","                            loss is being calculated. If True, then cross entropy loss\n","                            will also be added to the output loss.\n","        \n","        Returns:\n","            Either the predicted sequences with their scores for each element in the batch\n","            (if preds is True), or the loss value summed over all elements of the batch\n","            (if preds is False).\n","        \"\"\"\n","        tokenized_threads, labels = batch\n","        pad_mask = torch.where(tokenized_threads!=tokenizer.pad_token_id, torch.tensor(1).to(device), torch.tensor(0).to(device))\n","        x = transformer_model(input_ids=tokenized_threads,\n","                                                attention_mask=pad_mask,)\n","    #     print(tokenized_threads.shape)\n","    #     print(x[0].last_hidden_state, x[1].shape, len(x[2]))\n","        logits = linear_layer(transformer_model(input_ids=tokenized_threads,\n","                                                attention_mask=pad_mask,)[0][:, 0, :])\n","\n","        logits = normalizing_layer(logits)\n","        if(pred):\n","            return torch.argmax(logits, dim = 1)\n","\n","        ce_loss = cross_entropy_layer(logits, labels)\n","\n","        return ce_loss\n","\n","    import numpy as np\n","    def shuffle(data, labels):\n","        idx = np.random.permutation(len(data))\n","        x,y = np.array(data)[idx], np.array(labels)[idx]\n","        return x.tolist(), y.tolist()\n","\n","    BATCH_SIZE = 4\n","\n","    print(tokenizer.decode([101]))\n","\n","    print(device)\n","\n","    import pickle\n","\n","    linear_path = \"Model_longformer/linear_layer_2.pt\"\n","    cross_path = \"Model_longformer/cross_entropy_layer_2.pt\"\n","    tokenizer_path = \"Model_longformer/tokenizer_pre_2.pkl\"\n","    transformer_path = \"Model_longformer/transformer_layer_2.pt\"\n","    offset_path = \"Model_longformer/offset_2.pkl\"\n","\n","\n","\n","    train_dataset_data, train_dataset_label = shuffle(train_dataset_data, train_dataset_label)\n","    test_dataset_data, test_dataset_label = shuffle(test_dataset_data, test_dataset_label)\n","    X_train, Y_train = train_dataset_data, train_dataset_label\n","    X_test, Y_test = test_dataset_data, test_dataset_label\n","\n","    offset = 0\n","    with open(offset_path, \"wb\") as f:\n","        pickle.dump(offset, f)\n","    with open(offset_path, \"rb\") as f:\n","        offset = pickle.load(f)\n","\n","    n_epochs = 2\n","    step = 16000\n","    with open(offset_path, \"rb\") as f:\n","        offset = pickle.load(f)\n","\n","    for epoch in range(n_epochs):\n","        print(f\"------------EPOCH {epoch+1}---------------\")\n","        #loading data\n","        with open(offset_path, \"rb\") as f:\n","          offset = pickle.load(f)\n","        linear_layer.load_state_dict(torch.load(linear_path, map_location=device))\n","        cross_entropy_layer.load_state_dict(torch.load(cross_path, map_location=device))\n","        transformer_model.load_state_dict(torch.load(transformer_path , map_location=device))\n","        with open(tokenizer_path, \"rb\") as f:\n","            tokenizer = pickle.load(f)\n","\n","        while(offset <= len(X_train)):\n","\n","        #training and evaluating \n","            print(\"Train\")\n","            train(X_train[offset: offset + step], Y_train[offset: offset + step], BATCH_SIZE)\n","            torch.save(linear_layer.state_dict(), linear_path)\n","            torch.save(cross_entropy_layer.state_dict(), cross_path)\n","            torch.save(transformer_model.state_dict(), transformer_path)\n","            with open(tokenizer_path, \"wb\") as f:\n","                pickle.dump(tokenizer, f)\n","\n","        offset += step\n","        #saving offset\n","        with open(offset_path, \"wb\") as f:\n","            pickle.dump(offset, f)\n","            \n","        print(\"Test accuracy\")\n","        evaluate(X_test, Y_test, BATCH_SIZE)\n","\n","    print(offset)\n","\n","    print(len(test_dataset_label))\n","\n","    print(test_dataset_label)\n","\n","\n","\n","\n","\n","    torch.save(linear_layer.state_dict(), linear_path)\n","    torch.save(cross_entropy_layer.state_dict(), cross_path)\n","    torch.save(transformer_model.state_dict(), transformer_path)\n","    with open(tokenizer_path, \"wb\") as f:\n","        pickle.dump(tokenizer, f)\n","\n","    linear_layer.load_state_dict(torch.load(linear_path, map_location=device))\n","    cross_entropy_layer.load_state_dict(torch.load(cross_path, map_location=device))\n","    transformer_model.load_state_dict(torch.load(transformer_path , map_location=device))\n","    with open(tokenizer_path, \"rb\") as f:\n","        tokenizer = pickle.load(f)\n","\n","    from multiprocessing import Process, Manager, Array\n","    manager = Manager()\n","\n","    graph_attention = []\n","    for i in range(12):\n","        graph_attention.append({})\n","    mapping_ind = {}\n","    rev_mapping_ind = {}\n","\n","    cnt = 0\n","    threshold = 0.01\n","\n","    import multiprocessing as mp\n","    from tqdm import tqdm\n","\n","    import os\n","    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n","\n","    my_arr = [1 for i in range(100)]\n","\n","    def func(i, j , k):\n","        global my_arr\n","        my_arr[i] = 3\n","        \n","\n","    indexes = [(i, i, i) for i in range(12)]\n","    pool = mp.Pool(min(12, mp.cpu_count()))\n","    pool.starmap(func, indexes)\n","    pool.close()\n","\n","    type(mp.cpu_count())\n","\n","    temp_attention = []\n","    def init_arr(arr):\n","        globals()['arr'] = arr\n","    from copy import deepcopy\n","    def store_attention(lay, j, tokens):\n","            \n","        global mapping_ind, graph_attention, temp_attention;\n","        maximum_attention = np.max(temp_attention)\n","        weights = temp_attention[j, lay, :, :]\n","        temp_dict = {}\n","        for ind_1 in range(len(tokens)):\n","            for ind_2 in range(len(tokens)):\n","                if(tokens[ind_1] in [\"[NEWLINE]\", '[PAD]'] or tokens[ind_2] in [\"[NEWLINE]\", '[PAD]']):\n","                    continue\n","                node_1 = mapping_ind[tokens[ind_1]]\n","                node_2 = mapping_ind[tokens[ind_2]]\n","                weight = weights[ind_1, ind_2]\n","\n","                if(weight < threshold*maximum_attention):\n","                    continue\n","                if(node_1 not in temp_dict):\n","                    temp_dict[node_1] = {}\n","                \n","                if(node_2  not in temp_dict[node_1]):\n","                    temp_dict[node_1][node_2] = (weight, 1)\n","                else:\n","                    temp_dict[node_1][node_2] = (weight*temp_dict[node_1][node_2][1]/(temp_dict[node_1][node_2][1] + 1), temp_dict[node_1][node_2][1] + 1)\n","        graph_attention[lay] = deepcopy(temp_dict)\n","    #     print(graph_attention)\n","    def attention_graph(dataset_data, dataset_label, BATCH_SIZE):\n","        \n","        \n","        accumulate_over = 4\n","        global cnt, temp_attention, manager;\n","        optimizer.zero_grad()\n","\n","        for i, (tokenized_threads, labels) in tqdm(enumerate(generator(dataset_data, dataset_label, batch_size = BATCH_SIZE))):\n","        \n","            #Cast to PyTorch tensor\n","            tokenized_threads = torch.tensor(tokenized_threads, device=device)\n","            \n","            pad_mask = torch.where(tokenized_threads!=tokenizer.pad_token_id, 1, 0)\n","        \n","            attention = transformer_model(input_ids=tokenized_threads,\n","                                                attention_mask=pad_mask,)[-1][0].cpu().detach().numpy()\n","            temp_attention = attention[:, :, :, :]\n","            maximum_attention = np.max(attention)\n","            for j, tokenized_thread in enumerate(tokenized_threads):\n","                tokens = tokenizer.convert_ids_to_tokens(tokenized_thread) \n","                # print(len(tokens))\n","                # print(tokenized_thread.shape)\n","                for tok in tokens:\n","                    if(tok not in mapping_ind):\n","                        mapping_ind[tok] = cnt;\n","                        cnt += 1\n","                \n","        for i, (tokenized_threads, labels) in tqdm(enumerate(generator(dataset_data, dataset_label, batch_size = BATCH_SIZE))):\n","        \n","            #Cast to PyTorch tensor\n","            tokenized_threads = torch.tensor(tokenized_threads, device=device)\n","            \n","            pad_mask = torch.where(tokenized_threads!=tokenizer.pad_token_id, 1, 0)\n","        \n","            attention = transformer_model(input_ids=tokenized_threads,\n","                                                attention_mask=pad_mask,)[-1][0].cpu().detach().numpy()\n","            temp_attention = attention[:, :, :, :]\n","            maximum_attention = np.max(attention)\n","            for j, tokenized_thread in enumerate(tokenized_threads):\n","                tokens = tokenizer.convert_ids_to_tokens(tokenized_thread) \n","\n","    #             params = [(i1, j, tokens) for i1 in range(12)]\n","    #             pool = mp.Pool(min(12, mp.cpu_count()))\n","    #             pool.starmap(store_attention, params)\n","    #             pool.close()\n","                for lay in range(12):\n","                    weights = attention[j, lay, :, :]\n","                    for ind_1 in range(len(tokens)):\n","                        for ind_2 in range(len(tokens)):\n","                            if(tokens[ind_1] in [\"[NEWLINE]\", '[PAD]'] or tokens[ind_2] in [\"[NEWLINE]\", '[PAD]']):\n","                                continue\n","\n","                            node_1 = mapping_ind[tokens[ind_1]]\n","                            node_2 = mapping_ind[tokens[ind_2]]\n","                            weight = weights[ind_1, ind_2]\n","                            if(weight < threshold*maximum_attention):\n","                                continue\n","                            if(node_1 not in graph_attention[lay]):\n","                                graph_attention[lay][node_1] = {}\n","                            if(node_2  not in graph_attention[lay][node_1]):\n","                                graph_attention[lay][node_1][node_2] = (weight, 1)\n","                            else:\n","                                graph_attention[lay][node_1][node_2] = ((weight + (graph_attention[lay][node_1][node_2][0])*graph_attention[lay][node_1][node_2][1])/(graph_attention[lay][node_1][node_2][1] + 1), graph_attention[lay][node_1][node_2][1] + 1)\n","    #         print(cnt, cnt**2)\n","\n","    def extract_data(dataset_data, dataset_label, BATCH_SIZE):\n","        example = [[], [], []]\n","        for i, (tokenized_threads, labels) in enumerate(generator(dataset_data, dataset_label, batch_size = BATCH_SIZE)):\n","            for j in range(BATCH_SIZE):\n","                if(len(example[0]) < 5 or len(example[1]) < 5 or len(example[2]) < 5):\n","                    example[labels[j]].append((tokenizer.convert_ids_to_tokens(tokenized_threads[j]), tokenized_threads[j]))\n","                else:\n","                    break\n","        return example\n","\n","\n","\n","    text = extract_data(test_dataset_data, test_dataset_label, BATCH_SIZE)\n","\n","    import random\n","\n","    def find_degree(nodes, graph):\n","        out_degree = {}\n","        in_degree = {}\n","        for node_1 in graph:\n","            for node_2 in graph[node_1]:\n","                if(node_1 not in out_degree):\n","                    out_degree[node_1] = 0\n","                if(node_2 not in in_degree):\n","                    in_degree[node_2] = 0;\n","                in_degree[node_2] += 1;\n","                out_degree[node_1] += 1;\n","        ret_out = [out_degree[i] for i in nodes]\n","        ret_in = [in_degree[i] for i in nodes]\n","        return ret_out, ret_in\n","    def find_topk_tokens(graph, k):\n","        attention = {}\n","        attention_inward = {}\n","        for node_1 in graph:\n","            if(node_1 not in attention):\n","                attention[node_1] = (0, 0)\n","            for node_2 in graph[node_1]:\n","                \n","                if(node_2 not in attention_inward):\n","                    attention_inward[node_2] = (0, 0)\n","                attention[node_1] = ((graph[node_1][node_2][0] + (attention[node_1][0])*attention[node_1][1])/(attention[node_1][1] + 1), attention[node_1][1] + 1)\n","                attention_inward[node_2] = ((graph[node_1][node_2][0] + (attention_inward[node_2][0])*attention_inward[node_2][1])/(attention_inward[node_2][1] + 1), attention_inward[node_2][1] + 1)\n","        out_max = [i[0] for i in find_maxk(attention, k)]\n","        in_max = [i[0] for i in find_maxk(attention_inward, k)]\n","        return in_max, out_max, [i[1] for i in find_maxk(attention_inward, k)], [i[1] for i in find_maxk(attention, k)], find_mink(attention, k)[0][1], find_mink(attention_inward, k)[0][1]\n","    def find_mink(attention, k):\n","        ret = []\n","        for i in attention:\n","            if(i == mapping_ind[\"[CLS]\"] or i == mapping_ind[\"[SEP]\"]):\n","                continue\n","            ret.append((i, attention[i][0]))\n","        ret.sort(key = lambda x: x[1])\n","        return ret[:k]\n","    def find_maxk(attention, k):\n","        ret = []\n","        for i in attention:\n","            if(i == mapping_ind[\"[CLS]\"] or i == mapping_ind[\"[SEP]\"]):\n","                continue\n","            ret.append((i, attention[i][0]))\n","        ret.sort(key = lambda x: x[1])\n","        ret.reverse()\n","        # print(ret)\n","        return ret[:k]\n","\n","    def remove_pad(given_s):\n","        l = given_s.split()\n","        ret = []\n","        for i in l:\n","            if(i != \"[PAD]\"):\n","                ret.append(i)\n","        return \" \".join(ret)\n","\n","    def replacing(tok_mapping, tokenized_thread, tokens):\n","        ret_tokenized_thread = tokenized_thread.clone()\n","        ind = []\n","        for i in range(len(tokens)):\n","            if(mapping_ind[tokens[i]] in tok_mapping):\n","                ret_tokenized_thread[0, i] = tokenizer.encode(tok_mapping[mapping_ind[tokens[i]]])[1];\n","                ind.append(i)\n","        return ret_tokenized_thread, ind\n","\n","    vocab_dict = tokenizer.get_vocab()\n","\n","    vocab = []\n","    for i in vocab_dict:\n","        if(i not in [\"[CLS]\", '[PAD]', '[SEP]']):\n","            vocab.append(i)\n","\n","    import random\n","    def create_mapping(tok, ind):\n","        ret_dict = {}\n","        for k, i in enumerate(tok):\n","            if((k + i) < len(vocab)):\n","                ret_dict[i] = vocab[k + ind]\n","            else:\n","                ret_dict[i] = random.choice(vocab)\n","            ret_dict[i] = random.choice(vocab)\n","        return ret_dict\n","\n","    def get_embedding(ind, tokenized):\n","        pad_mask = torch.where(tokenized!=tokenizer.pad_token_id, 1, 0)\n","        embedding = transformer_model(input_ids=tokenized,attention_mask=pad_mask,).last_hidden_state\n","        ret_embedding = []\n","        for i in ind:\n","            ret_embedding.append(embedding[:, i, :].reshape((embedding.shape[2])))\n","        return ret_embedding\n","\n","    replacement = 100\n","    trials = 3\n","    trial_candidate = 5\n","    select_max = 5\n","\n","    print(len(text[1]))\n","\n","    import matplotlib.pyplot as plt\n","    from scipy import spatial\n","    text[0] = text[0][:3]\n","    text[1] = text[1][:3]\n","    text[2] = text[2][:3]\n","\n","    def predict_proba(tokenized_threads):\n","        pad_mask = torch.where(tokenized_threads!=tokenizer.pad_token_id, 1, 0)\n","        logits = linear_layer(transformer_model(input_ids=tokenized_threads,attention_mask=pad_mask,).last_hidden_state[:, 0, :])\n","        logits = torch.exp(logits)\n","        z = torch.sum(logits)\n","        logits = torch.divide(logits, z)\n","        return logits\n","    def mask_out_top5(text_lab, label, label_ind, flag, window_size):\n","        accumulate_over = 4\n","        global cnt, temp_attention, manager;\n","        optimizer.zero_grad()\n","        print(\"Window size : \", window_size)\n","        for i1, (tokens, tokenized_threads) in enumerate(text_lab):\n","            print(\"\\n\\n\\n\")\n","            print(\"###### Example :\", str(i1), \"label\", label, \"########\")\n","            #Cast to PyTorch tensor\n","            tokenized_threads = torch.tensor(tokenized_threads, device=device).reshape((1, len(tokenized_threads)))\n","            \n","            pad_mask = torch.where(tokenized_threads!=tokenizer.pad_token_id, 1, 0)\n","            \n","            attention = transformer_model(input_ids=tokenized_threads,\n","                                                attention_mask=pad_mask,)[-1][0].cpu().detach().numpy()\n","            graph = dict()\n","            for tok in tokens:\n","                if(tok not in mapping_ind):\n","                    mapping_ind[tok] = cnt;\n","                    cnt += 1\n","\n","            for lay in range(12):\n","                weights = attention[0, lay, :, :]\n","\n","                for ind_1 in range(len(tokens)):\n","                    for ind_2 in range(len(tokens)):\n","                        if(tokens[ind_1] in [\"[NEWLINE]\", '[PAD]'] or tokens[ind_2] in [\"[NEWLINE]\", '[PAD]']):\n","                            continue\n","                        node_1 = mapping_ind[tokens[ind_1]]\n","                        node_2 = mapping_ind[tokens[ind_2]]\n","                        rev_mapping_ind[node_1] = tokens[ind_1]\n","                        rev_mapping_ind[node_2] = tokens[ind_2]\n","                        weight = weights[ind_1, ind_2]\n","                        if(node_1 not in graph):\n","                            graph[node_1] = {}\n","                        if(node_2  not in graph[node_1]):\n","                            graph[node_1][node_2] = (weight, 1)\n","                        else:\n","                            graph[node_1][node_2] = ((weight + (graph[node_1][node_2][0])*graph[node_1][node_2][1])/(graph[node_1][node_2][1] + 1), graph[node_1][node_2][1] + 1)\n","\n","            topk_in, topk_out, topk_in_attention, topk_out_attention, topk_in_min, topk_out_min  = find_topk_tokens(graph, select_max)\n","\n","            \n","            candidate = random.sample(topk_in, trial_candidate)\n","            for j in range(trials):\n","                random.shuffle(vocab)\n","                print([ rev_mapping_ind[z] for z in candidate], \"\\n\")\n","                in_logits_l2_norm = []\n","                for l in tqdm(range(0, min(len(vocab), 1000), select_max)):\n","                    top5_in_mapping = create_mapping(candidate, l)\n","\n","                    example_in, ind_in = replacing(top5_in_mapping, tokenized_threads, tokens)\n","\n","                    actual_in = get_embedding(ind_in, tokenized_threads)\n","                    replaced_in = get_embedding(ind_in, example_in)\n","\n","                    if(len(ind_in) >= 1):\n","                        final_actual_in = torch.divide(sum(actual_in),len(actual_in)).cpu().detach().numpy()\n","                        final_replaced_in = torch.divide(sum(replaced_in) , len(replaced_in)).cpu().detach().numpy()\n","                    else:\n","                        continue\n","                    if(predict_proba(example_in).cpu().detach().numpy().tolist()[0][label_ind] < 0.5):\n","                        continue\n","                    subtract_in = np.subtract(final_actual_in, final_replaced_in)\n","                    if(flag == \"L2\"):\n","                        in_logits_l2_norm.append((np.linalg.norm(subtract_in), predict_proba(example_in).cpu().detach().numpy().tolist()[0][label_ind]))\n","                    elif(flag == \"Cosine\"):\n","                        in_logits_l2_norm.append((spatial.distance.cosine(final_actual_in, final_replaced_in), predict_proba(example_in).cpu().detach().numpy().tolist()[0][label_ind]))\n","                        \n","                        \n","                in_logits_l2_norm.sort(key = lambda x : x[0])\n","                \n","\n","                plt.scatter([i[0] for i in in_logits_l2_norm],[i[1] for i in in_logits_l2_norm])\n","            print(\"| Min:\", topk_in_min)\n","            for i in range(len(topk_in)):\n","                print(\"Token:\", rev_mapping_ind[topk_in[i]], \"| attention:\", topk_in_attention[i])\n","            plt.title('Example ' + str(i) + ' in')\n","            plt.xlabel(flag + ' norm')\n","            plt.ylabel('probability of '+ label )\n","            plt.show()\n","            plt.clf()\n","            \n","            candidate = random.sample(topk_out, trial_candidate)\n","            for j in range(trials):\n","\n","                random.shuffle(vocab)\n","                print([ rev_mapping_ind[z] for z in candidate], \"\\n\")\n","                out_logits_l2_norm = []\n","                for l in tqdm(range(0, min(len(vocab), 1000), select_max)):\n","                    top5_out_mapping = create_mapping([j], l)\n","\n","                    \n","                    \n","                    example_out, ind_out = replacing(top5_out_mapping, tokenized_threads, tokens)\n","\n","                    actual_out = get_embedding(ind_out, tokenized_threads)\n","                    replaced_out = get_embedding(ind_out, example_out)\n","    #                 print(sum(actual_out))\n","                    if(len(ind_out) >= 1):\n","                        final_actual_out = torch.divide(sum(actual_out) , len(actual_out)).cpu().detach().numpy()\n","                        final_replaced_out = torch.divide(sum(replaced_out) , len(replaced_out)).cpu().detach().numpy()\n","                    else:\n","                        continue\n","                    if(predict_proba(example_out).cpu().detach().numpy().tolist()[0][label_ind] < 0.50):\n","                        continue    \n","                    subtract_out = np.subtract(final_actual_out, final_replaced_out)\n","                    if(flag == \"L2\"):\n","                        out_logits_l2_norm.append((np.linalg.norm(subtract_out), predict_proba(example_out).cpu().detach().numpy().tolist()[0][label_ind]))\n","                    elif(flag == \"Cosine\"):\n","                        out_logits_l2_norm.append((spatial.distance.cosine(final_actual_out, final_replaced_out), predict_proba(example_out).cpu().detach().numpy().tolist()[0][label_ind]))\n","                        \n","                out_logits_l2_norm.sort(key = lambda x : x[0])\n","                \n","                plt.scatter([i[0] for i in out_logits_l2_norm],[i[1] for i in out_logits_l2_norm])\n","\n","            print(\"| Min:\", topk_out_min)\n","            for i in range(len(topk_out)):\n","                print(\"Token:\", rev_mapping_ind[topk_out[i]], \"| attention:\", topk_out_attention[i])\n","            plt.title('Example ' + str(i) + ' out')\n","            plt.xlabel(flag + ' norm')\n","            plt.ylabel('probability of ' + label )\n","            plt.show()\n","            plt.clf()\n","\n","    mask_out_top5(text[0], \"neutral\", 0, \"L2\")\n","\n","    mask_out_top5(text[1], \"entailment\", 1, \"L2\",)\n","\n","    mask_out_top5(text[2], \"contradiction\", 2, \"L2\",)\n","\n","    mask_out_top5(text[0], \"neutral\", 0, \"Cosine\")\n","\n","    mask_out_top5(text[1], \"entailment\", 1, \"Cosine\",)\n","\n","    mask_out_top5(text[2], \"contradiction\", 2, \"Cosine\",)\n","\n","    select_max_imp = 100\n","\n","    def calculate_imp(true_logits, true_class, perturbed_logits, perturbed_class):\n","        if(perturbed_class == true_class):\n","            return true_logits[true_class] - perturbed_logits[true_class]\n","        return true_logits[true_class] - perturbed_logits[true_class] + true_logits[perturbed_class] - perturbed_logits[perturbed_class]\n","\n","    print(tokenizer.decode([1532]))\n","\n","    def intersection(a, b):\n","        cnt = 0;\n","        for i in range(min(len(a), len(b))):\n","            if(a[i] == b[i]):\n","                cnt += 1;\n","            \n","        return (cnt/min(len(a), len(b)))*100\n","\n","    def find_intersection_top5(text_lab, label, label_ind, flag):\n","        accumulate_over = 4\n","        global cnt, temp_attention, manager;\n","        optimizer.zero_grad()\n","\n","        for i, (tokens, tokenized_threads) in enumerate(text_lab):\n","            print(\"\\n\\n\\n\")\n","            print(\"###### Example :\", str(i), \"label\", label, \"########\")\n","            #Cast to PyTorch tensor\n","            tokenized_threads = torch.tensor(tokenized_threads, device=device).reshape((1, len(tokenized_threads)))\n","            \n","            pad_mask = torch.where(tokenized_threads!=tokenizer.pad_token_id, 1, 0)\n","            \n","            attention = transformer_model(input_ids=tokenized_threads,\n","                                                attention_mask=pad_mask,)[-1][0].cpu().detach().numpy()\n","            graph = dict()\n","            \n","            for tok in tokens:\n","                if(tok not in mapping_ind):\n","                    mapping_ind[tok] = cnt;\n","                    cnt += 1\n","\n","            for lay in range(12):\n","                weights = attention[0, lay, :, :]\n","                for ind_1 in range(len(tokens)):\n","                    for ind_2 in range(len(tokens)):\n","                        if(tokens[ind_1] in [\"[NEWLINE]\", '[PAD]'] or tokens[ind_2] in [\"[NEWLINE]\", '[PAD]']):\n","                            continue\n","                        node_1 = mapping_ind[tokens[ind_1]]\n","                        node_2 = mapping_ind[tokens[ind_2]]\n","                        rev_mapping_ind[node_1] = tokens[ind_1]\n","                        rev_mapping_ind[node_2] = tokens[ind_2]\n","                        weight = weights[ind_1, ind_2]\n","                        if(node_1 not in graph):\n","                            graph[node_1] = {}\n","                        if(node_2  not in graph[node_1]):\n","                            graph[node_1][node_2] = (weight, 1)\n","                        else:\n","                            graph[node_1][node_2] = ((weight + (graph[node_1][node_2][0])*graph[node_1][node_2][1])/(graph[node_1][node_2][1] + 1), graph[node_1][node_2][1] + 1)\n","            logits = predict_proba(tokenized_threads)\n","            class_assigned = torch.argmax(logits[0, :])\n","            topk_in, topk_out, topk_in_attention, topk_out_attention, topk_in_min, topk_out_min = find_topk_tokens(graph, select_max_imp)\n","            arr = tokenized_threads.cpu().detach().clone().numpy()\n","\n","            candidates = []\n","            for j in range(len(tokens)):\n","                temp_tokenized_threads = np.concatenate([arr[0, : j], arr[0, j + 1:], np.array([tokenizer.pad_token_id])])\n","            \n","                temp_logits = predict_proba(torch.tensor(temp_tokenized_threads, device=device).reshape((1, len(temp_tokenized_threads))))\n","\n","                temp_class_assigned = torch.argmax(temp_logits)\n","\n","                importance_score = calculate_imp(logits[0], class_assigned, temp_logits[0], temp_class_assigned)\n","                candidates.append((arr[0, j], importance_score))\n","            candidates.sort(key = lambda x : x[1])\n","            candidates.reverse()\n","            print(candidates)\n","            print(\"intersection between in : \", intersection([tokenizer.decode(i[0]) for i in candidates[:select_max_imp] if tokenizer.decode(i[0]) not in [\"[CLS]\", \"[PAD]\", \"[SEP]\"]], [rev_mapping_ind[z] for z in topk_in]))\n","            print(\"intersection between out : \",intersection([tokenizer.decode(i[0]) for i in candidates[:select_max_imp] if tokenizer.decode(i[0]) not in [\"[CLS]\", \"[PAD]\",\"[SEP]\"]], [rev_mapping_ind[z] for z in topk_out]))\n","\n","    find_intersection_top5(text[2], \"contradiction\", 2, \"Cosine\",)\n","\n","    find_intersection_top5(text[1], \"entailment\", 1, \"L2\",)\n","\n","    find_intersection_top5(text[0], \"neutral\", 0, \"L2\")"]}]}