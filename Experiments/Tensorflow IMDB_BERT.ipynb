{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":2906,"status":"ok","timestamp":1641066060191,"user":{"displayName":"Anunay Yadav","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7WujoeIi9pQg7KsDLqvOvz6TK-Yx_MZE9mnJUzw=s64","userId":"18135409234894719745"},"user_tz":-330},"id":"a0d6b7da"},"outputs":[],"source":["import os\n","from operator import itemgetter    \n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import warnings\n","warnings.filterwarnings('ignore')\n","get_ipython().magic(u'matplotlib inline')\n","plt.style.use('ggplot')\n","\n","import tensorflow as tf\n","\n","from keras import models, regularizers, layers, optimizers, losses, metrics\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from tensorflow.keras.utils import to_categorical\n"," \n","from keras.datasets import imdb"],"id":"a0d6b7da"},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vOjLmSnDdhcE","executionInfo":{"status":"ok","timestamp":1641066065562,"user_tz":-330,"elapsed":5377,"user":{"displayName":"Anunay Yadav","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7WujoeIi9pQg7KsDLqvOvz6TK-Yx_MZE9mnJUzw=s64","userId":"18135409234894719745"}},"outputId":"3c48d4cb-1083-4997-d5d8-9337effc73cc"},"id":"vOjLmSnDdhcE","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":6187,"status":"ok","timestamp":1641066071740,"user":{"displayName":"Anunay Yadav","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7WujoeIi9pQg7KsDLqvOvz6TK-Yx_MZE9mnJUzw=s64","userId":"18135409234894719745"},"user_tz":-330},"id":"efaa5bf3"},"outputs":[],"source":["\n","(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n","num_words=10000)"],"id":"efaa5bf3"},{"cell_type":"code","source":["print(train_data)\n","print(train_labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pPzHHKfCVvZb","executionInfo":{"status":"ok","timestamp":1641066071741,"user_tz":-330,"elapsed":28,"user":{"displayName":"Anunay Yadav","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7WujoeIi9pQg7KsDLqvOvz6TK-Yx_MZE9mnJUzw=s64","userId":"18135409234894719745"}},"outputId":"c474f9fd-f0eb-4c6c-e786-1dd6fb1f251e"},"id":"pPzHHKfCVvZb","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["[list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32])\n"," list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 8255, 2, 349, 2637, 148, 605, 2, 8003, 15, 123, 125, 68, 2, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 2, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 2, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95])\n"," list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113])\n"," ...\n"," list([1, 11, 6, 230, 245, 6401, 9, 6, 1225, 446, 2, 45, 2174, 84, 8322, 4007, 21, 4, 912, 84, 2, 325, 725, 134, 2, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 2, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 9406, 1209, 2295, 2, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 2, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 2, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 2, 5, 27, 710, 117, 2, 8123, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 2, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 2, 7750, 5, 4241, 18, 4, 8497, 2, 250, 11, 1818, 7561, 4, 4217, 5408, 747, 1115, 372, 1890, 1006, 541, 9303, 7, 4, 59, 2, 4, 3586, 2])\n"," list([1, 1446, 7079, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 2, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 2, 40, 319, 5872, 112, 6700, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 7200, 4, 2, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 2, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 2, 12, 38, 84, 80, 124, 12, 9, 23])\n"," list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 2, 270, 2, 5, 2, 2, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 9245, 9, 24, 6, 78, 1099, 17, 2345, 2, 21, 27, 9685, 6139, 5, 2, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 6789, 2, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 7585, 8, 2197, 2, 2, 544, 5, 383, 1271, 848, 1468, 2, 497, 2, 8, 1597, 8778, 2, 21, 60, 27, 239, 9, 43, 8368, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])]\n","[1 0 0 ... 0 1 0]\n"]}]},{"cell_type":"code","execution_count":5,"metadata":{"id":"254c61cc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1641066071742,"user_tz":-330,"elapsed":6,"user":{"displayName":"Anunay Yadav","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7WujoeIi9pQg7KsDLqvOvz6TK-Yx_MZE9mnJUzw=s64","userId":"18135409234894719745"}},"outputId":"88b6cd8b-ce89-44b7-e9a0-6dcc2d66a7e8"},"outputs":[{"output_type":"stream","name":"stdout","text":["? beautiful and touching movie rich colors great settings good acting and one of the most charming movies i have seen in a while i never saw such an interesting setting when i was in china my wife liked it so much she asked me to ? on and rate it so other would enjoy too\n"]}],"source":["word_index = imdb.get_word_index()\n","\n","reverse_word_index = dict(\n","[(value, key) for (key, value) in word_index.items()])\n","\n","decoded_review = ' '.join(\n","[reverse_word_index.get(i - 3, '?') for i in train_data[123]])\n","\n","print(decoded_review)"],"id":"254c61cc"},{"cell_type":"code","source":["model_version = 'bert-base-cased'\n","import torch\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"bKVRDGmkden6","executionInfo":{"status":"ok","timestamp":1641066072830,"user_tz":-330,"elapsed":1092,"user":{"displayName":"Anunay Yadav","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7WujoeIi9pQg7KsDLqvOvz6TK-Yx_MZE9mnJUzw=s64","userId":"18135409234894719745"}}},"id":"bKVRDGmkden6","execution_count":6,"outputs":[]},{"cell_type":"code","source":["ac_dict = {\"positive\" : 1, \"negative\" : 0}"],"metadata":{"id":"JRdIbk_xF6RV","executionInfo":{"status":"ok","timestamp":1641066072831,"user_tz":-330,"elapsed":6,"user":{"displayName":"Anunay Yadav","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7WujoeIi9pQg7KsDLqvOvz6TK-Yx_MZE9mnJUzw=s64","userId":"18135409234894719745"}}},"id":"JRdIbk_xF6RV","execution_count":7,"outputs":[]},{"cell_type":"code","source":["import torch.nn as nn"],"metadata":{"id":"1WZoh-syGCjs","executionInfo":{"status":"ok","timestamp":1641066072832,"user_tz":-330,"elapsed":5,"user":{"displayName":"Anunay Yadav","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7WujoeIi9pQg7KsDLqvOvz6TK-Yx_MZE9mnJUzw=s64","userId":"18135409234894719745"}}},"id":"1WZoh-syGCjs","execution_count":8,"outputs":[]},{"cell_type":"code","source":["\n","%%capture\n","from transformers import AutoTokenizer, AutoModel\n","tokenizer = AutoTokenizer.from_pretrained(model_version,\n","                                          bos_token = \"[CLS]\",\n","                                          eos_token = \"[SEP]\")\n","transformer_model = AutoModel.from_pretrained(model_version, output_attentions = True).to(device)\n","linear_layer = nn.Linear(transformer_model.config.hidden_size,\n","                         len(ac_dict)).to(device)\n","cross_entropy_layer = nn.CrossEntropyLoss()"],"metadata":{"id":"nSyUVUHRdZgI","executionInfo":{"status":"ok","timestamp":1641066081641,"user_tz":-330,"elapsed":8168,"user":{"displayName":"Anunay Yadav","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7WujoeIi9pQg7KsDLqvOvz6TK-Yx_MZE9mnJUzw=s64","userId":"18135409234894719745"}}},"id":"nSyUVUHRdZgI","execution_count":9,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm"],"metadata":{"id":"eWwKx-dLd6Nx","executionInfo":{"status":"ok","timestamp":1641066081641,"user_tz":-330,"elapsed":20,"user":{"displayName":"Anunay Yadav","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7WujoeIi9pQg7KsDLqvOvz6TK-Yx_MZE9mnJUzw=s64","userId":"18135409234894719745"}}},"id":"eWwKx-dLd6Nx","execution_count":10,"outputs":[]},{"cell_type":"code","source":["!pip install datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SisR5maqKL-A","executionInfo":{"status":"ok","timestamp":1641066091427,"user_tz":-330,"elapsed":9804,"user":{"displayName":"Anunay Yadav","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7WujoeIi9pQg7KsDLqvOvz6TK-Yx_MZE9mnJUzw=s64","userId":"18135409234894719745"}},"outputId":"6911faf3-9681-4640-f1ec-3bb44eeddbc6"},"id":"SisR5maqKL-A","execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.17.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.2)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.11.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.2.1)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.6)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.8)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (5.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"]}]},{"cell_type":"code","source":["from typing import List, Tuple"],"metadata":{"id":"vx6d-O4sdmn8","executionInfo":{"status":"ok","timestamp":1641066091428,"user_tz":-330,"elapsed":10,"user":{"displayName":"Anunay Yadav","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7WujoeIi9pQg7KsDLqvOvz6TK-Yx_MZE9mnJUzw=s64","userId":"18135409234894719745"}}},"id":"vx6d-O4sdmn8","execution_count":12,"outputs":[]},{"cell_type":"code","execution_count":13,"metadata":{"id":"18776582","executionInfo":{"status":"ok","timestamp":1641066092071,"user_tz":-330,"elapsed":651,"user":{"displayName":"Anunay Yadav","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7WujoeIi9pQg7KsDLqvOvz6TK-Yx_MZE9mnJUzw=s64","userId":"18135409234894719745"}}},"outputs":[],"source":["def pad_batch(elems: List[List[int]], pad_token_id: int) -> List[List[int]]:\n","    \"\"\"Pads all lists in elems to the maximum list length of any list in \n","    elems. Pads with pad_token_id.\n","    \"\"\"\n","    max_len = max([len(elem) for elem in elems])\n","    return [elem+[pad_token_id]*(max_len-len(elem)) for elem in elems]\n","\n","def load_dataset(tokenizer):\n","  (train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n","  word_index = imdb.get_word_index()\n","\n","  reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n","\n","  new_train_data = []\n","  for tokenized_text in tqdm(train_data):\n","    decoded_sent = \" \".join([reverse_word_index.get(word - 3, \"?\") for word in tokenized_text])\n","    # print(decoded_sent)\n","    new_encoded_text = tokenizer.encode(decoded_sent)\n","    new_train_data.append(new_encoded_text)\n","  \n","  new_test_data = []\n","  for tokenized_text in tqdm(test_data):\n","    decoded_sent = \" \".join([reverse_word_index[word] for word in tokenized_text])\n","    new_encoded_text = tokenizer.encode(decoded_sent)\n","    new_test_data.append(new_encoded_text)\n","\n","  return (new_train_data, train_labels), (new_test_data, test_labels)\n","\n","def generator(dataset_data, dataset_label, max_len = 512, batch_size = 16):\n","  i = 0\n","  tokenized_threads, labels = [], []\n","  while i<len(dataset_data):\n","    tokenized_threads.append(dataset_data[i][:max_len])\n","    labels.append(dataset_label[i])\n","    i += 1\n","        \n","    if i%batch_size==0:\n","      yield (pad_batch(tokenized_threads, tokenizer.pad_token_id), \n","                  labels)\n","      tokenized_threads, labels = [], []"],"id":"18776582"},{"cell_type":"code","source":["from itertools import chain\n","\n","import torch.optim as optim\n","\n","optimizer = optim.Adam(params = chain(transformer_model.parameters(),\n","                                      linear_layer.parameters()),\n","                       lr = 2e-5,)"],"metadata":{"id":"bguPdT7MFuU7","executionInfo":{"status":"ok","timestamp":1641066092072,"user_tz":-330,"elapsed":6,"user":{"displayName":"Anunay Yadav","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7WujoeIi9pQg7KsDLqvOvz6TK-Yx_MZE9mnJUzw=s64","userId":"18135409234894719745"}}},"id":"bguPdT7MFuU7","execution_count":14,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"MOKEorH3Hbjm","executionInfo":{"status":"ok","timestamp":1641066092074,"user_tz":-330,"elapsed":8,"user":{"displayName":"Anunay Yadav","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7WujoeIi9pQg7KsDLqvOvz6TK-Yx_MZE9mnJUzw=s64","userId":"18135409234894719745"}}},"id":"MOKEorH3Hbjm","execution_count":14,"outputs":[]},{"cell_type":"code","source":["(train_dataset, train_labels), (test_dataset, test_labels) = load_dataset(tokenizer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mblvixkpiA8Y","executionInfo":{"status":"ok","timestamp":1641066148130,"user_tz":-330,"elapsed":56063,"user":{"displayName":"Anunay Yadav","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7WujoeIi9pQg7KsDLqvOvz6TK-Yx_MZE9mnJUzw=s64","userId":"18135409234894719745"}},"outputId":"f93cfd1c-6ea2-45ee-9a27-ffb00d5a2b25"},"id":"mblvixkpiA8Y","execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 25000/25000 [00:29<00:00, 840.55it/s]\n","100%|██████████| 25000/25000 [00:20<00:00, 1238.99it/s]\n"]}]},{"cell_type":"code","source":["BATCH_SIZE = 16"],"metadata":{"id":"A0nCCwhZH7vr","executionInfo":{"status":"ok","timestamp":1641066149138,"user_tz":-330,"elapsed":1030,"user":{"displayName":"Anunay Yadav","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7WujoeIi9pQg7KsDLqvOvz6TK-Yx_MZE9mnJUzw=s64","userId":"18135409234894719745"}}},"id":"A0nCCwhZH7vr","execution_count":16,"outputs":[]},{"cell_type":"code","source":["def train(dataset_data, dataset_label, batch_size):\n","    global values_weight, values_bias;\n","    accumulate_over = 4\n","    \n","    optimizer.zero_grad()\n","    print(\"Training\")\n","    for i, (tokenized_threads, labels) in enumerate(generator(dataset_data, dataset_label,  batch_size = batch_size)):\n","        \n","        #Cast to PyTorch tensor\n","        tokenized_threads = torch.tensor(tokenized_threads, device=device)\n","        labels = torch.tensor(labels, device=device, dtype=torch.long)\n","\n","\n","        loss = compute((tokenized_threads, \n","                        labels,), False) / batch_size\n","        \n","        print(\"Loss: \", loss)\n","\n","        loss.backward()\n","\n","        if i%accumulate_over==accumulate_over-1:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","    \n","    optimizer.step()"],"metadata":{"id":"5T3gEFcjHcFQ","executionInfo":{"status":"ok","timestamp":1641066149140,"user_tz":-330,"elapsed":9,"user":{"displayName":"Anunay Yadav","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7WujoeIi9pQg7KsDLqvOvz6TK-Yx_MZE9mnJUzw=s64","userId":"18135409234894719745"}}},"id":"5T3gEFcjHcFQ","execution_count":17,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Iwrtb65RK9cy","executionInfo":{"status":"ok","timestamp":1641066149140,"user_tz":-330,"elapsed":8,"user":{"displayName":"Anunay Yadav","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7WujoeIi9pQg7KsDLqvOvz6TK-Yx_MZE9mnJUzw=s64","userId":"18135409234894719745"}}},"id":"Iwrtb65RK9cy","execution_count":17,"outputs":[]},{"cell_type":"code","source":["from datasets import load_metric\n","metric1 = load_metric(\"accuracy\")\n","metric3 = load_metric(\"precision\")\n","metric4 = load_metric(\"f1\")\n","metric2 = load_metric(\"recall\")\n"],"metadata":{"id":"qaQg6WTUJY0M","executionInfo":{"status":"ok","timestamp":1641066149649,"user_tz":-330,"elapsed":517,"user":{"displayName":"Anunay Yadav","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7WujoeIi9pQg7KsDLqvOvz6TK-Yx_MZE9mnJUzw=s64","userId":"18135409234894719745"}}},"id":"qaQg6WTUJY0M","execution_count":18,"outputs":[]},{"cell_type":"code","source":["def evaluate(dataset_data, dataset_label, BATCH_SIZE):\n","    \n","    int_to_labels = {v:k for k, v in ac_dict.items()}\n","    print('Evaluation')\n","    \n","    with torch.no_grad():\n","        for i, (tokenized_threads, labels) in enumerate(generator(dataset_data, dataset_label, batch_size = BATCH_SIZE)):\n","            # print(comp_type_labels)\n","            #Cast to PyTorch tensor\n","            tokenized_threads = torch.tensor(tokenized_threads, device=device)\n","            labels = torch.tensor(labels, device=device)\n","\n","            preds = compute((tokenized_threads, \n","                            labels,), pred=True)\n","            \n","            \n","            metric1.add_batch(predictions=preds, \n","                            references=labels,)\n","                            #tokenized_threads=tokenized_threads.cpu().tolist())\n","            metric2.add_batch(predictions=preds, \n","                            references=labels,)\n","            metric3.add_batch(predictions=preds, \n","                            references=labels,)\n","            metric4.add_batch(predictions=preds, \n","                            references=labels,)\n","            break\n","        \n","    print(metric1.compute())\n","    print(metric2.compute())\n","    print(metric3.compute())\n","    print(metric4.compute())"],"metadata":{"id":"CFW0TinkIGC-","executionInfo":{"status":"ok","timestamp":1641066149650,"user_tz":-330,"elapsed":4,"user":{"displayName":"Anunay Yadav","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7WujoeIi9pQg7KsDLqvOvz6TK-Yx_MZE9mnJUzw=s64","userId":"18135409234894719745"}}},"id":"CFW0TinkIGC-","execution_count":19,"outputs":[]},{"cell_type":"code","source":["\n","(train_dataset_data, train_dataset_label), (test_dataset_data, test_dataset_label) = load_dataset(tokenizer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wICMiB86Mdc6","executionInfo":{"status":"ok","timestamp":1641066204858,"user_tz":-330,"elapsed":55211,"user":{"displayName":"Anunay Yadav","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7WujoeIi9pQg7KsDLqvOvz6TK-Yx_MZE9mnJUzw=s64","userId":"18135409234894719745"}},"outputId":"ce110db7-6bc6-4bff-90d8-26fdf176f13f"},"id":"wICMiB86Mdc6","execution_count":20,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 25000/25000 [00:28<00:00, 870.45it/s] \n","100%|██████████| 25000/25000 [00:20<00:00, 1237.88it/s]\n"]}]},{"cell_type":"code","source":["def compute(batch: Tuple[torch.Tensor, torch.Tensor], pred: bool=True):\n","    \"\"\"\n","    Args:\n","        batch:  A tuple having tokenized thread of shape [batch_size, seq_len],\n","                component type labels of shape [batch_size, seq_len], and a global\n","                attention mask for Longformer, of the same shape.\n","        \n","        \n","        cross_entropy:  This argument will only be used if preds=False, i.e., if \n","                        loss is being calculated. If True, then cross entropy loss\n","                        will also be added to the output loss.\n","    \n","    Returns:\n","        Either the predicted sequences with their scores for each element in the batch\n","        (if preds is True), or the loss value summed over all elements of the batch\n","        (if preds is False).\n","    \"\"\"\n","    tokenized_threads, labels = batch\n","    pad_mask = torch.where(tokenized_threads!=tokenizer.pad_token_id, 1, 0)\n","    print(transformer_model(input_ids=tokenized_threads,\n","                                            attention_mask=pad_mask,).last_hidden_state.shape)\n","    logits = linear_layer(transformer_model(input_ids=tokenized_threads,\n","                                            attention_mask=pad_mask,).last_hidden_state)[:, 0, :]\n","    if(pred):\n","      return torch.argmax(logits, dim = 1)\n","\n","\n","    ce_loss = cross_entropy_layer(logits, labels)\n","\n","    return ce_loss"],"metadata":{"id":"O2H2YiwbEqQB","executionInfo":{"status":"ok","timestamp":1641066204859,"user_tz":-330,"elapsed":20,"user":{"displayName":"Anunay Yadav","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7WujoeIi9pQg7KsDLqvOvz6TK-Yx_MZE9mnJUzw=s64","userId":"18135409234894719745"}}},"id":"O2H2YiwbEqQB","execution_count":21,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","def shuffle(data, labels):\n","  idx = np.random.permutation(len(data))\n","  x,y = np.array(data)[idx], np.array(labels)[idx]\n","  return x.tolist(), y.tolist()"],"metadata":{"id":"Lm2WsTYLIAdm","executionInfo":{"status":"ok","timestamp":1641066204860,"user_tz":-330,"elapsed":19,"user":{"displayName":"Anunay Yadav","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7WujoeIi9pQg7KsDLqvOvz6TK-Yx_MZE9mnJUzw=s64","userId":"18135409234894719745"}}},"id":"Lm2WsTYLIAdm","execution_count":22,"outputs":[]},{"cell_type":"code","source":["n_epochs = 35\n","for epoch in range(n_epochs):\n","    print(f\"------------EPOCH {epoch+1}---------------\")\n","    train_dataset_data, train_dataset_label = shuffle(train_dataset_data, train_dataset_label)\n","    test_dataset_data, test_dataset_label = shuffle(test_dataset_data, test_dataset_label)\n","    train(train_dataset_data, train_dataset_label, BATCH_SIZE)\n","    evaluate(test_dataset_data, test_dataset_label, BATCH_SIZE)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":554},"id":"SVxjjXP5Kou5","executionInfo":{"status":"error","timestamp":1641066221233,"user_tz":-330,"elapsed":16391,"user":{"displayName":"Anunay Yadav","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7WujoeIi9pQg7KsDLqvOvz6TK-Yx_MZE9mnJUzw=s64","userId":"18135409234894719745"}},"outputId":"e5d483f8-fc28-489b-b1a8-c28e7b14af19"},"id":"SVxjjXP5Kou5","execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["------------EPOCH 1---------------\n","Training\n","torch.Size([16, 512, 768])\n","Loss:  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n","torch.Size([16, 512, 768])\n","Loss:  tensor(0.0394, device='cuda:0', grad_fn=<DivBackward0>)\n","torch.Size([16, 512, 768])\n","Loss:  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n","torch.Size([16, 366, 768])\n","Loss:  tensor(0.0439, device='cuda:0', grad_fn=<DivBackward0>)\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-c8ba759a1d24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_dataset_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtest_dataset_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-0650b65c07af>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset_data, dataset_label, batch_size)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         loss = compute((tokenized_threads, \n\u001b[0;32m---> 15\u001b[0;31m                         labels,), False) / batch_size\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-21-39212887f588>\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(batch, pred)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mpad_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_threads\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     print(transformer_model(input_ids=tokenized_threads,\n\u001b[0;32m---> 21\u001b[0;31m                                             attention_mask=pad_mask,).last_hidden_state.shape)\n\u001b[0m\u001b[1;32m     22\u001b[0m     logits = linear_layer(transformer_model(input_ids=tokenized_threads,\n\u001b[1;32m     23\u001b[0m                                             attention_mask=pad_mask,).last_hidden_state)[:, 0, :]\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1007\u001b[0m         )\n\u001b[1;32m   1008\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    590\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m                 )\n\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m         layer_output = apply_chunking_to_forward(\n\u001b[0;32m--> 514\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m         )\n\u001b[1;32m    516\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   2368\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2370\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mgelu\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m   1554\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_unary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1556\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 0; 11.17 GiB total capacity; 9.80 GiB already allocated; 21.81 MiB free; 10.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Tensorflow IMDB_BERT.ipynb","provenance":[{"file_id":"1F47EPalEIm9sIEXUVvtZIbEzJjWMwibc","timestamp":1641035306025}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}