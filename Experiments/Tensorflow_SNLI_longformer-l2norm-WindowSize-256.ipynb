{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"fRebVJARzCHR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646741439909,"user_tz":-330,"elapsed":3554,"user":{"displayName":"Anunay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12399266251701703896"}},"outputId":"a29f0531-5d9d-4f0d-f5cc-1857e397a150"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"fRebVJARzCHR"},{"cell_type":"code","source":[""],"metadata":{"id":"M1lGeygbTb6c"},"id":"M1lGeygbTb6c","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"6i-RGCA-TqUO"},"id":"6i-RGCA-TqUO","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"OIEXyEuzT49u"},"id":"OIEXyEuzT49u","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"dVZ67CI2UHnU"},"id":"dVZ67CI2UHnU","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"qxxePTn4UWQv"},"id":"qxxePTn4UWQv","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"JxSzhS81Uk6S"},"id":"JxSzhS81Uk6S","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"4BxGewnSUzjv"},"id":"4BxGewnSUzjv","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"k3GkE5CcVCNi"},"id":"k3GkE5CcVCNi","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"8b5gcxcuVQ2u"},"id":"8b5gcxcuVQ2u","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fqdiJ-vOzoBH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646741439913,"user_tz":-330,"elapsed":13,"user":{"displayName":"Anunay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12399266251701703896"}},"outputId":"52b1ff60-8f79-455e-c135-075aa4919b1f"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1CLIDh5rSBKopppgMh7zb8C3_AWNoz6B8/Bert_Lime/SNLI\n"]}],"source":["%cd /content/drive/MyDrive/Bert_Lime/SNLI/"],"id":"fqdiJ-vOzoBH"},{"cell_type":"code","execution_count":null,"metadata":{"id":"e63e6ba0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646741469850,"user_tz":-330,"elapsed":29946,"user":{"displayName":"Anunay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12399266251701703896"}},"outputId":"41a8ede3-19be-4202-a388-57437bb9cb03"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: seqeval in /usr/local/lib/python3.7/dist-packages (1.2.2)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.18.4)\n","Requirement already satisfied: allennlp in /usr/local/lib/python3.7/dist-packages (2.9.0)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.21.5)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.0.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.63.0)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.2.0)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.2)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.4.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n","Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.2)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n","Requirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.5)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.6.4)\n","Requirement already satisfied: spacy<3.3,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.2.4)\n","Requirement already satisfied: nltk<3.6.6 in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.2.5)\n","Requirement already satisfied: cached-path<2.0.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.1.0)\n","Requirement already satisfied: fairscale==0.4.5 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.4.5)\n","Requirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.1.0)\n","Requirement already satisfied: jsonnet>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.18.0)\n","Requirement already satisfied: wandb<0.13.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.12.11)\n","Requirement already satisfied: torch<1.11.0,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from allennlp) (1.10.0+cu111)\n","Requirement already satisfied: base58 in /usr/local/lib/python3.7/dist-packages (from allennlp) (2.1.1)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.1.96)\n","Requirement already satisfied: transformers<4.16,>=4.1 in /usr/local/lib/python3.7/dist-packages (from allennlp) (4.15.0)\n","Requirement already satisfied: torchvision<0.12.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.11.1+cu111)\n","Requirement already satisfied: checklist==0.0.11 in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.0.11)\n","Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from allennlp) (0.99)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp) (3.1.0)\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from allennlp) (8.12.0)\n","Requirement already satisfied: munch>=2.5 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp) (2.5.0)\n","Requirement already satisfied: ipywidgets>=7.5 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp) (7.6.5)\n","Requirement already satisfied: patternfork-nosql in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp) (3.6)\n","Requirement already satisfied: iso-639 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp) (0.4.5)\n","Requirement already satisfied: jupyter>=1.0 in /usr/local/lib/python3.7/dist-packages (from checklist==0.0.11->allennlp) (1.0.0)\n","Requirement already satisfied: boto3<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from cached-path<2.0.0,>=1.0.2->allennlp) (1.21.14)\n","Requirement already satisfied: google-cloud-storage<3.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from cached-path<2.0.0,>=1.0.2->allennlp) (1.18.1)\n","Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.0->cached-path<2.0.0,>=1.0.2->allennlp) (0.5.2)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.0->cached-path<2.0.0,>=1.0.2->allennlp) (0.10.0)\n","Requirement already satisfied: botocore<1.25.0,>=1.24.14 in /usr/local/lib/python3.7/dist-packages (from boto3<2.0,>=1.0->cached-path<2.0.0,>=1.0.2->allennlp) (1.24.14)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.25.0,>=1.24.14->boto3<2.0,>=1.0->cached-path<2.0.0,>=1.0.2->allennlp) (2.8.2)\n","Requirement already satisfied: google-auth>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<3.0,>=1.0->cached-path<2.0.0,>=1.0.2->allennlp) (1.35.0)\n","Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<3.0,>=1.0->cached-path<2.0.0,>=1.0.2->allennlp) (1.0.3)\n","Requirement already satisfied: google-resumable-media<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage<3.0,>=1.0->cached-path<2.0.0,>=1.0.2->allennlp) (0.4.1)\n","Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2.0->google-cloud-storage<3.0,>=1.0->cached-path<2.0.0,>=1.0.2->allennlp) (57.4.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2.0->google-cloud-storage<3.0,>=1.0->cached-path<2.0.0,>=1.0.2->allennlp) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2.0->google-cloud-storage<3.0,>=1.0->cached-path<2.0.0,>=1.0.2->allennlp) (4.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2.0->google-cloud-storage<3.0,>=1.0->cached-path<2.0.0,>=1.0.2->allennlp) (4.2.4)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.2.0->google-cloud-storage<3.0,>=1.0->cached-path<2.0.0,>=1.0.2->allennlp) (1.15.0)\n","Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage<3.0,>=1.0->cached-path<2.0.0,>=1.0.2->allennlp) (1.26.3)\n","Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage<3.0,>=1.0->cached-path<2.0.0,>=1.0.2->allennlp) (1.55.0)\n","Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage<3.0,>=1.0->cached-path<2.0.0,>=1.0.2->allennlp) (3.17.3)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage<3.0,>=1.0->cached-path<2.0.0,>=1.0.2->allennlp) (2018.9)\n","Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.5.0)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (1.0.2)\n","Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.1.1)\n","Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.2.0)\n","Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (4.10.1)\n","Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.1.3)\n","Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5->checklist==0.0.11->allennlp) (3.5.2)\n","Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.1.1)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.3.5)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.7.5)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.8.1)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (2.6.1)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (1.0.18)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (4.8.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (4.4.2)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp) (5.6.1)\n","Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp) (5.3.1)\n","Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp) (5.2.0)\n","Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter>=1.0->checklist==0.0.11->allennlp) (5.2.2)\n","Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (4.3.3)\n","Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (4.9.2)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.18.1)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (21.4.0)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (5.4.0)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (3.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets>=7.5->checklist==0.0.11->allennlp) (0.2.5)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2.0->google-cloud-storage<3.0,>=1.0->cached-path<2.0.0,>=1.0.2->allennlp) (0.4.8)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3,>=2.1.0->allennlp) (2.0.6)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3,>=2.1.0->allennlp) (1.0.6)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3,>=2.1.0->allennlp) (1.0.0)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3,>=2.1.0->allennlp) (7.4.0)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3,>=2.1.0->allennlp) (1.0.5)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3,>=2.1.0->allennlp) (3.0.6)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3,>=2.1.0->allennlp) (0.4.1)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3,>=2.1.0->allennlp) (1.1.3)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3,>=2.1.0->allennlp) (0.9.0)\n","Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision<0.12.0,>=0.8.1->allennlp) (7.1.2)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<4.16,>=4.1->allennlp) (0.0.47)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.16,>=4.1->allennlp) (2019.12.20)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.16,>=4.1->allennlp) (0.10.3)\n","Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (3.1.27)\n","Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.5.7)\n","Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.0.8)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (5.4.8)\n","Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (0.1.2)\n","Requirement already satisfied: yaspin>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (2.1.0)\n","Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (2.3)\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.2.2)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (0.4.0)\n","Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (7.1.2)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp) (4.0.9)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp) (5.0.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (2.11.3)\n","Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (0.13.1)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (1.8.0)\n","Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.5->checklist==0.0.11->allennlp) (22.3.0)\n","Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (0.7.0)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->allennlp) (1.5.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter>=1.0->checklist==0.0.11->allennlp) (2.0.1)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (1.5.0)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.4)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (4.1.0)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.7.1)\n","Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.6.0)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.8.4)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter>=1.0->checklist==0.0.11->allennlp) (0.5.1)\n","Requirement already satisfied: python-docx in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp) (0.8.11)\n","Requirement already satisfied: feedparser in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp) (6.0.8)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp) (0.16.0)\n","Requirement already satisfied: cherrypy in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp) (18.6.1)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp) (4.2.6)\n","Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp) (20211012)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp) (4.6.3)\n","Requirement already satisfied: backports.csv in /usr/local/lib/python3.7/dist-packages (from patternfork-nosql->checklist==0.0.11->allennlp) (1.0.7)\n","Requirement already satisfied: portend>=2.1.1 in /usr/local/lib/python3.7/dist-packages (from cherrypy->patternfork-nosql->checklist==0.0.11->allennlp) (3.1.0)\n","Requirement already satisfied: zc.lockfile in /usr/local/lib/python3.7/dist-packages (from cherrypy->patternfork-nosql->checklist==0.0.11->allennlp) (2.0)\n","Requirement already satisfied: cheroot>=8.2.1 in /usr/local/lib/python3.7/dist-packages (from cherrypy->patternfork-nosql->checklist==0.0.11->allennlp) (8.6.0)\n","Requirement already satisfied: jaraco.collections in /usr/local/lib/python3.7/dist-packages (from cherrypy->patternfork-nosql->checklist==0.0.11->allennlp) (3.5.1)\n","Requirement already satisfied: jaraco.functools in /usr/local/lib/python3.7/dist-packages (from cheroot>=8.2.1->cherrypy->patternfork-nosql->checklist==0.0.11->allennlp) (3.5.0)\n","Requirement already satisfied: tempora>=1.8 in /usr/local/lib/python3.7/dist-packages (from portend>=2.1.1->cherrypy->patternfork-nosql->checklist==0.0.11->allennlp) (5.0.1)\n","Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.7/dist-packages (from feedparser->patternfork-nosql->checklist==0.0.11->allennlp) (1.0.0)\n","Requirement already satisfied: jaraco.classes in /usr/local/lib/python3.7/dist-packages (from jaraco.collections->cherrypy->patternfork-nosql->checklist==0.0.11->allennlp) (3.2.1)\n","Requirement already satisfied: jaraco.text in /usr/local/lib/python3.7/dist-packages (from jaraco.collections->cherrypy->patternfork-nosql->checklist==0.0.11->allennlp) (3.7.0)\n","Requirement already satisfied: jaraco.context>=4.1 in /usr/local/lib/python3.7/dist-packages (from jaraco.text->jaraco.collections->cherrypy->patternfork-nosql->checklist==0.0.11->allennlp) (4.1.1)\n","Requirement already satisfied: cryptography in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp) (36.0.1)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp) (1.15.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six->patternfork-nosql->checklist==0.0.11->allennlp) (2.21)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (1.11.0)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (1.4.0)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp) (0.7.1)\n","Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter>=1.0->checklist==0.0.11->allennlp) (2.0.1)\n","Requirement already satisfied: flax in /usr/local/lib/python3.7/dist-packages (0.4.0)\n","Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from flax) (1.21.5)\n","Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from flax) (1.0.3)\n","Requirement already satisfied: optax in /usr/local/lib/python3.7/dist-packages (from flax) (0.1.1)\n","Requirement already satisfied: jax>=0.2.21 in /usr/local/lib/python3.7/dist-packages (from flax) (0.3.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from flax) (3.2.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.21->flax) (3.10.0.2)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.21->flax) (1.0.0)\n","Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.21->flax) (1.4.1)\n","Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.21->flax) (3.3.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py->jax>=0.2.21->flax) (1.15.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (2.8.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (3.0.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (0.11.0)\n","Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.7/dist-packages (from optax->flax) (0.3.0+cuda11.cudnn805)\n","Requirement already satisfied: chex>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from optax->flax) (0.1.1)\n","Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax->flax) (0.1.6)\n","Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax->flax) (0.11.2)\n","Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from jaxlib>=0.1.37->optax->flax) (2.0)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.5)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n"]}],"source":["!pip install transformers\n","!pip install seqeval datasets allennlp\n","!pip install flax\n","!pip install sentencepiece\n","!pip install nltk\n","!pip install gensim"],"id":"e63e6ba0"},{"cell_type":"code","source":["import tensorflow_datasets as tfds\n","from tensorflow_datasets.text import Snli\n","import os"],"metadata":{"id":"Fmqxrd80z9y5"},"id":"Fmqxrd80z9y5","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from __future__ import division\n","\n","import torch\n","import torch.nn as nn\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","class Sparsemax(nn.Module):\n","    \"\"\"Sparsemax function.\"\"\"\n","\n","    def __init__(self, dim=None):\n","        \"\"\"Initialize sparsemax activation\n","        \n","        Args:\n","            dim (int, optional): The dimension over which to apply the sparsemax function.\n","        \"\"\"\n","        super(Sparsemax, self).__init__()\n","\n","        self.dim = -1 if dim is None else dim\n","\n","    def forward(self, input):\n","        \"\"\"Forward function.\n","        Args:\n","            input (torch.Tensor): Input tensor. First dimension should be the batch size\n","        Returns:\n","            torch.Tensor: [batch_size x number_of_logits] Output tensor\n","        \"\"\"\n","        # Sparsemax currently only handles 2-dim tensors,\n","        # so we reshape to a convenient shape and reshape back after sparsemax\n","        input = input.transpose(0, self.dim)\n","        original_size = input.size()\n","        input = input.reshape(input.size(0), -1)\n","        input = input.transpose(0, 1)\n","        dim = 1\n","\n","        number_of_logits = input.size(dim)\n","\n","        # Translate input by max for numerical stability\n","        input = input - torch.max(input, dim=dim, keepdim=True)[0].expand_as(input)\n","\n","        # Sort input in descending order.\n","        # (NOTE: Can be replaced with linear time selection method described here:\n","        # http://stanford.edu/~jduchi/projects/DuchiShSiCh08.html)\n","        zs = torch.sort(input=input, dim=dim, descending=True)[0]\n","        range = torch.arange(start=1, end=number_of_logits + 1, step=1, device=device, dtype=input.dtype).view(1, -1)\n","        range = range.expand_as(zs)\n","\n","        # Determine sparsity of projection\n","        bound = 1 + range * zs\n","        cumulative_sum_zs = torch.cumsum(zs, dim)\n","        is_gt = torch.gt(bound, cumulative_sum_zs).type(input.type())\n","        k = torch.max(is_gt * range, dim, keepdim=True)[0]\n","\n","        # Compute threshold function\n","        zs_sparse = is_gt * zs\n","\n","        # Compute taus\n","        taus = (torch.sum(zs_sparse, dim, keepdim=True) - 1) / k\n","        taus = taus.expand_as(input)\n","\n","        # Sparsemax\n","        self.output = torch.max(torch.zeros_like(input), input - taus)\n","\n","        # Reshape back to original shape\n","        output = self.output\n","        output = output.transpose(0, 1)\n","        output = output.reshape(original_size)\n","        output = output.transpose(0, self.dim)\n","\n","        return output\n","\n","    def backward(self, grad_output):\n","        \"\"\"Backward function.\"\"\"\n","        dim = 1\n","\n","        nonzeros = torch.ne(self.output, 0)\n","        sum = torch.sum(grad_output * nonzeros, dim=dim) / torch.sum(nonzeros, dim=dim)\n","        self.grad_input = nonzeros * (grad_output - sum.expand_as(grad_output))\n","\n","        return self.grad_input"],"metadata":{"id":"mqLmGUarOOCr"},"id":"mqLmGUarOOCr","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[""],"metadata":{"id":"64WmTY09mFwk"},"id":"64WmTY09mFwk"},{"cell_type":"code","source":["os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"],"metadata":{"id":"9u_eCaLRl1lz"},"id":"9u_eCaLRl1lz","execution_count":null,"outputs":[]},{"cell_type":"code","source":["ac_dict = {\"neutral\" : 1, \"entailment\" : 0, \"contradiction\" : 2}"],"metadata":{"id":"eqoCMH9638pk"},"id":"eqoCMH9638pk","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a0d6b7da"},"outputs":[],"source":["import os\n","from operator import itemgetter    \n","import numpy as np\n","# import pandas as pd\n","import matplotlib.pyplot as plt\n","import warnings\n","warnings.filterwarnings('ignore')\n","get_ipython().magic(u'matplotlib inline')\n","plt.style.use('ggplot')\n","\n","# from keras import models, regularizers, layers, optimizers, losses, metrics\n","# from keras.models import Sequential\n","# from keras.layers import Dense\n","# from tensorflow.keras.utils import to_categorical\n","# from keras.layers import LayerNormalization\n","import tensorflow as tf\n","from tensorflow.keras.datasets import imdb"],"id":"a0d6b7da"},{"cell_type":"code","execution_count":null,"metadata":{"id":"b092aa8b"},"outputs":[],"source":["import torch"],"id":"b092aa8b"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1646741470747,"user":{"displayName":"Anunay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12399266251701703896"},"user_tz":-330},"id":"cd39d439","outputId":"0a64039d-621f-4310-c7d9-96b687f9b962"},"outputs":[{"output_type":"stream","name":"stdout","text":["True\n"]}],"source":["print(torch.cuda.is_available())"],"id":"cd39d439"},{"cell_type":"code","source":["print(len(ac_dict))"],"metadata":{"id":"NP8yCaTBhZZj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646741470747,"user_tz":-330,"elapsed":12,"user":{"displayName":"Anunay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12399266251701703896"}},"outputId":"804c2352-2e25-47be-b107-51c2890f6c16"},"id":"NP8yCaTBhZZj","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["3\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bKVRDGmkden6"},"outputs":[],"source":["model_version = 'allenai/longformer-base-4096'\n","import torch\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"],"id":"bKVRDGmkden6"},{"cell_type":"code","execution_count":null,"metadata":{"id":"392b0df8"},"outputs":[],"source":["import torch.nn as nn\n","import torch"],"id":"392b0df8"},{"cell_type":"code","source":[""],"metadata":{"id":"vAXlVzyNDG_h"},"id":"vAXlVzyNDG_h","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cae186b8"},"outputs":[],"source":["from huggingface_hub import create_repo"],"id":"cae186b8"},{"cell_type":"code","source":["from transformers import LongformerTokenizer, LongformerModel\n"],"metadata":{"id":"mq4NOMbKXhuw"},"id":"mq4NOMbKXhuw","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4344,"status":"ok","timestamp":1646741475083,"user":{"displayName":"Anunay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12399266251701703896"},"user_tz":-330},"id":"nSyUVUHRdZgI","outputId":"eb92aa76-7aea-47d3-8d2e-b54aef019d84"},"outputs":[{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n","- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["\n","from transformers import LongformerConfig, LongformerTokenizer, LongformerModel\n","\n","tokenizer = LongformerTokenizer.from_pretrained(model_version,\n","                                          bos_token = \"[CLS]\",\n","                                          eos_token = \"[SEP]\")\n","transformer_model = LongformerModel.from_pretrained(model_version, attention_window = 256, output_attentions = True).to(device)\n","linear_layer = nn.Linear(transformer_model.config.hidden_size,\n","                         len(ac_dict)).to(device)\n","cross_entropy_layer = nn.CrossEntropyLoss()\n","normalizing_layer = Sparsemax(dim = 1)"],"id":"nSyUVUHRdZgI"},{"cell_type":"code","source":[""],"metadata":{"id":"3f0tWE9LcGtc"},"id":"3f0tWE9LcGtc","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"nTKNTceccVXA"},"id":"nTKNTceccVXA","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # del transformer_model\n","# del optimizer"],"metadata":{"id":"cMCmJ87Yj1AX"},"id":"cMCmJ87Yj1AX","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1646741475085,"user":{"displayName":"Anunay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12399266251701703896"},"user_tz":-330},"id":"75f214cb","outputId":"4272501b-c048-4499-9e93-e00d731c221e"},"outputs":[{"output_type":"stream","name":"stdout","text":["768\n"]}],"source":["print(transformer_model.config.hidden_size)"],"id":"75f214cb"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1646741475086,"user":{"displayName":"Anunay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12399266251701703896"},"user_tz":-330},"id":"5aa87c3d","outputId":"0a165597-1956-4136-deb6-cabc7fc6db9d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{},"execution_count":47}],"source":["device"],"id":"5aa87c3d"},{"cell_type":"code","execution_count":null,"metadata":{"id":"eWwKx-dLd6Nx"},"outputs":[],"source":["from tqdm import tqdm"],"id":"eWwKx-dLd6Nx"},{"cell_type":"code","execution_count":null,"metadata":{"id":"vx6d-O4sdmn8"},"outputs":[],"source":["from typing import List, Tuple"],"id":"vx6d-O4sdmn8"},{"cell_type":"code","execution_count":null,"metadata":{"id":"18776582"},"outputs":[],"source":["def pad_batch(elems: List[List[int]], pad_token_id: int) -> List[List[int]]:\n","    \"\"\"Pads all lists in elems to the maximum list length of any list in \n","    elems. Pads with pad_token_id.\n","    \"\"\"\n","    max_len = max([len(elem) for elem in elems])\n","    return [elem+[pad_token_id]*(max_len-len(elem)) for elem in elems]\n","\n","def load_dataset(tokenizer):\n","  train, test = tfds.as_numpy(tfds.load(\"Snli\", split=['train', 'test']))\n","\n","  parts = []\n","\n","  new_train_data = []\n","  train_labels = []\n","  cnt = 0\n","  for tokenized_text in tqdm(train):\n","    cnt += 1\n","    # if(cnt >= 10000):\n","      # break\n","    decoded_sent = \"[CLS] \" +  str(tokenized_text[\"hypothesis\"])[2:-1] + \" [SEP] \" + str(tokenized_text[\"premise\"])[2:-1] + \" [SEP]\"\n","    # print(decoded_sent)\n","    new_encoded_text = tokenizer.encode(decoded_sent)\n","    if(tokenized_text[\"label\"] < 0 or tokenized_text[\"label\"] > 2):\n","      continue\n","    new_train_data.append(new_encoded_text)\n","    train_labels.append(tokenized_text[\"label\"])\n","  \n","  new_test_data = []\n","  test_labels = []\n","  for tokenized_text in tqdm(test):\n","    decoded_sent = \"[CLS] \" +  str(tokenized_text[\"hypothesis\"])[2:-1] + \" [SEP] \" + str(tokenized_text[\"premise\"])[2:-1] + \" [SEP]\"\n","    # print(decoded_sent)\n","    new_encoded_text = tokenizer.encode(decoded_sent)\n","    if(tokenized_text[\"label\"] < 0 or tokenized_text[\"label\"] > 2):\n","      continue\n","    new_test_data.append(new_encoded_text)\n","    test_labels.append(tokenized_text[\"label\"])\n","\n","\n","  return (new_train_data, train_labels), (new_test_data, test_labels)\n","\n","def generator(dataset_data, dataset_label, max_len = 512, batch_size = 16):\n","  i = 0\n","  tokenized_threads, labels = [], []\n","  while i<len(dataset_data):\n","    tokenized_threads.append(dataset_data[i][:max_len])\n","    labels.append(dataset_label[i])\n","    i += 1\n","        \n","    if i%batch_size==0:\n","      yield (pad_batch(tokenized_threads, tokenizer.pad_token_id), \n","                  labels)\n","      tokenized_threads, labels = [], []"],"id":"18776582"},{"cell_type":"code","execution_count":null,"metadata":{"id":"bguPdT7MFuU7"},"outputs":[],"source":["from itertools import chain\n","\n","import torch.optim as optim\n","\n","optimizer = optim.Adam(params = chain(transformer_model.parameters(),\n","                                      linear_layer.parameters()),\n","                       lr = 2e-5)"],"id":"bguPdT7MFuU7"},{"cell_type":"code","execution_count":null,"metadata":{"id":"MOKEorH3Hbjm"},"outputs":[],"source":[""],"id":"MOKEorH3Hbjm"},{"cell_type":"code","execution_count":null,"metadata":{"id":"mblvixkpiA8Y"},"outputs":[],"source":[""],"id":"mblvixkpiA8Y"},{"cell_type":"code","execution_count":null,"metadata":{"id":"A0nCCwhZH7vr"},"outputs":[],"source":["train_loss_list = []\n","test_loss_list = []\n","stats_list = []"],"id":"A0nCCwhZH7vr"},{"cell_type":"code","execution_count":null,"metadata":{"id":"5T3gEFcjHcFQ"},"outputs":[],"source":["def train(dataset_data, dataset_label, batch_size):\n","    global values_weight, values_bias, train_loss_list;\n","    accumulate_over = 32\n","    \n","    optimizer.zero_grad()\n","    print(\"Training\")\n","    for i, (tokenized_threads, labels) in enumerate(generator(dataset_data, dataset_label,  batch_size = batch_size)):\n","        \n","        #Cast to PyTorch tensor\n","        tokenized_threads = torch.tensor(tokenized_threads, device=device)\n","        labels = torch.tensor(labels, device=device, dtype=torch.long)\n","\n","\n","        loss = compute((tokenized_threads, \n","                        labels,), False)\n","\n","        print(\"\\rLoss: \", loss.item(), end = \" \")\n","        train_loss_list.append(loss.item())\n","        loss.backward()\n","\n","        if i%accumulate_over==accumulate_over-1:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","    print()\n","    optimizer.step()"],"id":"5T3gEFcjHcFQ"},{"cell_type":"code","execution_count":null,"metadata":{"id":"qaQg6WTUJY0M"},"outputs":[],"source":["from datasets import load_metric\n","metric1 = load_metric(\"accuracy\")\n","metric3 = load_metric(\"precision\", average = \"micro\")\n","metric2 = load_metric(\"f1\", average = \"micro\")\n","metric4 = load_metric(\"recall\", average = \"micro\")\n","metric5 = load_metric(\"precision\", average = \"macro\")\n","metric6 = load_metric(\"f1\", average = \"macro\")\n","metric7 = load_metric(\"recall\", average = \"macro\")\n"],"id":"qaQg6WTUJY0M"},{"cell_type":"code","execution_count":null,"metadata":{"id":"CFW0TinkIGC-"},"outputs":[],"source":["def evaluate(dataset_data, dataset_label, BATCH_SIZE):\n","    global test_loss_list, stats_list;\n","    int_to_labels = {v:k for k, v in ac_dict.items()}\n","    print('Evaluation')\n","    \n","    with torch.no_grad():\n","        for i, (tokenized_threads, labels) in enumerate(generator(dataset_data, dataset_label, batch_size = BATCH_SIZE)):\n","            # print(comp_type_labels)\n","            #Cast to PyTorch tensor\n","            tokenized_threads = torch.tensor(tokenized_threads, device=device)\n","            labels = torch.tensor(labels, device=device)\n","\n","            preds = compute((tokenized_threads, \n","                            labels,), pred=True)\n","            loss = compute((tokenized_threads, labels,), pred = False)\n","            \n","            metric1.add_batch(predictions=preds, \n","                            references=labels,)\n","                            #tokenized_threads=tokenized_threads.cpu().tolist())\n","            metric2.add_batch(predictions=preds, \n","                            references=labels,)\n","            metric3.add_batch(predictions=preds, \n","                            references=labels,)\n","            metric4.add_batch(predictions=preds, \n","                            references=labels,)\n","            metric5.add_batch(predictions=preds, \n","                            references=labels,)\n","            metric6.add_batch(predictions=preds, \n","                            references=labels,)\n","            metric7.add_batch(predictions=preds, \n","                            references=labels,)\n","            \n","            print(\"\\rLoss: \", loss.item(), end = \" \")\n","            test_loss_list.append(loss.item())\n","\n","    print()\n","    stats = []\n","    stats.append(metric1.compute())\n","    stats.append(metric5.compute(average = \"macro\"))\n","    stats.append(metric6.compute(average = \"macro\"))\n","    stats.append(metric7.compute(average = \"macro\"))\n","    print(stats[0])\n","    print(\"Micro\")\n","    print(metric2.compute(average = \"micro\"))\n","    print(metric3.compute(average = \"micro\"))\n","    print(metric4.compute(average = \"micro\"))\n","    print(\"Macro\")\n","    print(stats[1])\n","    print(stats[2])\n","    print(stats[3])\n","    stats_list.append(stats)"],"id":"CFW0TinkIGC-"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wICMiB86Mdc6","outputId":"1550fe01-7b7e-4563-9d31-eb00a60b33c4","executionInfo":{"status":"ok","timestamp":1646742067539,"user_tz":-330,"elapsed":590964,"user":{"displayName":"Anunay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12399266251701703896"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:absl:Load dataset info from /root/tensorflow_datasets/snli/1.1.0\n","INFO:absl:Reusing dataset snli (/root/tensorflow_datasets/snli/1.1.0)\n","INFO:absl:Constructing tf.data.Dataset for split ['train', 'test'], from /root/tensorflow_datasets/snli/1.1.0\n","550152it [09:39, 948.63it/s]\n","10000it [00:10, 979.25it/s]\n"]}],"source":["\n","(train_dataset_data, train_dataset_label), (test_dataset_data, test_dataset_label) = load_dataset(tokenizer)"],"id":"wICMiB86Mdc6"},{"cell_type":"code","source":["transformer_model.resize_token_embeddings(len(tokenizer))"],"metadata":{"id":"cfOvGJd3htke","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646742068149,"user_tz":-330,"elapsed":632,"user":{"displayName":"Anunay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12399266251701703896"}},"outputId":"c68bbcfb-c895-454e-d615-56eedf00493f"},"id":"cfOvGJd3htke","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Embedding(50267, 768)"]},"metadata":{},"execution_count":57}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O2H2YiwbEqQB"},"outputs":[],"source":["def compute(batch: Tuple[torch.Tensor, torch.Tensor], pred: bool=True):\n","    \"\"\"\n","    Args:\n","        batch:  A tuple having tokenized thread of shape [batch_size, seq_len],\n","                component type labels of shape [batch_size, seq_len], and a global\n","                attention mask for Longformer, of the same shape.\n","        \n","        \n","        cross_entropy:  This argument will only be used if preds=False, i.e., if \n","                        loss is being calculated. If True, then cross entropy loss\n","                        will also be added to the output loss.\n","    \n","    Returns:\n","        Either the predicted sequences with their scores for each element in the batch\n","        (if preds is True), or the loss value summed over all elements of the batch\n","        (if preds is False).\n","    \"\"\"\n","    tokenized_threads, labels = batch\n","    pad_mask = torch.where(tokenized_threads!=tokenizer.pad_token_id, torch.tensor(1).to(device), torch.tensor(0).to(device))\n","    x = transformer_model(input_ids=tokenized_threads,\n","                                            attention_mask=pad_mask,)\n","#     print(tokenized_threads.shape)\n","#     print(x[0].last_hidden_state, x[1].shape, len(x[2]))\n","    logits = linear_layer(transformer_model(input_ids=tokenized_threads,\n","                                            attention_mask=pad_mask,)[0][:, 0, :])\n","\n","    logits = normalizing_layer(logits)\n","    if(pred):\n","      return torch.argmax(logits, dim = 1)\n","\n","    ce_loss = cross_entropy_layer(logits, labels)\n","\n","    return ce_loss"],"id":"O2H2YiwbEqQB"},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lm2WsTYLIAdm"},"outputs":[],"source":["import numpy as np\n","def shuffle(data, labels):\n","  idx = np.random.permutation(len(data))\n","  x,y = np.array(data)[idx], np.array(labels)[idx]\n","  return x.tolist(), y.tolist()"],"id":"Lm2WsTYLIAdm"},{"cell_type":"code","execution_count":null,"metadata":{"id":"f929079c"},"outputs":[],"source":["BATCH_SIZE = 4"],"id":"f929079c"},{"cell_type":"code","execution_count":null,"metadata":{"id":"aa4794a9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646742068150,"user_tz":-330,"elapsed":15,"user":{"displayName":"Anunay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12399266251701703896"}},"outputId":"69cbe245-5bf5-4c85-86df-1606ffdf180b"},"outputs":[{"output_type":"stream","name":"stdout","text":[" like\n"]}],"source":["print(tokenizer.decode([101]))"],"id":"aa4794a9"},{"cell_type":"code","execution_count":null,"metadata":{"id":"b440f040","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646742068150,"user_tz":-330,"elapsed":9,"user":{"displayName":"Anunay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12399266251701703896"}},"outputId":"ae147b93-6aad-4d12-f403-aa78b26092c7"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n"]}],"source":["print(device)"],"id":"b440f040"},{"cell_type":"code","source":["import pickle\n"],"metadata":{"id":"-fqry_h8jw66"},"id":"-fqry_h8jw66","execution_count":null,"outputs":[]},{"cell_type":"code","source":["linear_path = \"Model_longformer/linear_layer_256.pt\"\n","cross_path = \"Model_longformer/cross_entropy_layer_256.pt\"\n","tokenizer_path = \"Model_longformer/tokenizer_pre_256.pkl\"\n","transformer_path = \"Model_longformer/transformer_layer_256.pt\"\n","offset_path = \"Model_longformer/offset_256.pkl\"\n","data_path = \"Model_longformer/data_256.pkl\""],"metadata":{"id":"AynuMCQZ22R4"},"id":"AynuMCQZ22R4","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Z042d0if2rUC"},"id":"Z042d0if2rUC","execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset_data, train_dataset_label = shuffle(train_dataset_data, train_dataset_label)\n","test_dataset_data, test_dataset_label = shuffle(test_dataset_data, test_dataset_label)\n","X_train, Y_train = train_dataset_data, train_dataset_label\n","X_test, Y_test = test_dataset_data, test_dataset_label"],"metadata":{"id":"ky-mqjie3FRW"},"id":"ky-mqjie3FRW","execution_count":null,"outputs":[]},{"cell_type":"code","source":["offset = 0\n","with open(offset_path, \"wb\") as f:\n","    pickle.dump(offset, f)\n","with open(offset_path, \"rb\") as f:\n","    offset = pickle.load(f)\n","torch.save(linear_layer.state_dict(), linear_path)\n","torch.save(cross_entropy_layer.state_dict(), cross_path)\n","torch.save(transformer_model.state_dict(), transformer_path)\n","with open(data_path, \"wb\") as f:\n","  pickle.dump([train_loss_list, test_loss_list, stats_list], f)\n","with open(tokenizer_path, \"wb\") as f:\n","  pickle.dump(tokenizer, f)"],"metadata":{"id":"Ha1-QBS7x3JD"},"id":"Ha1-QBS7x3JD","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SVxjjXP5Kou5","scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"outputId":"702f060e-944b-4e7f-9a36-bce7f3a43919"},"outputs":[{"output_type":"stream","name":"stdout","text":["------------EPOCH 1---------------\n","Train\n","Training\n","Loss:  0.5514447689056396 \n","Test accuracy\n","Evaluation\n","Loss:  0.8014447689056396 \n","{'accuracy': 0.8232899022801303}\n","Micro\n","{'f1': 0.8232899022801303}\n","{'precision': 0.8232899022801303}\n","{'recall': 0.8232899022801303}\n","Macro\n","{'precision': 0.823103660679109}\n","{'f1': 0.8204934813977225}\n","{'recall': 0.8222195015074206}\n","Train\n","Training\n","Loss:  0.5514447689056396 \n","Test accuracy\n","Evaluation\n","Loss:  0.8014447689056396 \n","{'accuracy': 0.846192996742671}\n","Micro\n","{'f1': 0.846192996742671}\n","{'precision': 0.846192996742671}\n","{'recall': 0.846192996742671}\n","Macro\n","{'precision': 0.8468188138644283}\n","{'f1': 0.844753123635821}\n","{'recall': 0.8451225864398499}\n","Train\n","Training\n","Loss:  0.5514447689056396 "]}],"source":["n_epochs = 1\n","step = 16000\n","with open(offset_path, \"rb\") as f:\n","      offset = pickle.load(f)\n","\n","for epoch in range(n_epochs):\n","    print(f\"------------EPOCH {epoch+1}---------------\")\n","    #loading data\n","    with open(offset_path, \"rb\") as f:\n","      offset = pickle.load(f)\n","    linear_layer.load_state_dict(torch.load(linear_path, map_location=device))\n","    cross_entropy_layer.load_state_dict(torch.load(cross_path, map_location=device))\n","    transformer_model.load_state_dict(torch.load(transformer_path , map_location=device))\n","    with open(tokenizer_path, \"rb\") as f:\n","      tokenizer = pickle.load(f)\n","    with open(data_path, \"rb\") as f:\n","      data = pickle.load(f)\n","      stats_list = data[2]\n","      train_loss_list = data[0]\n","      test_loss_list = data[1]\n","    while(offset <= len(X_train)):\n","\n","      #training and evaluating \n","      print(\"Train\")\n","      train(X_train[offset: offset + step], Y_train[offset: offset + step], BATCH_SIZE)\n","      torch.save(linear_layer.state_dict(), linear_path)\n","      torch.save(cross_entropy_layer.state_dict(), cross_path)\n","      torch.save(transformer_model.state_dict(), transformer_path)\n","      \n","      with open(data_path, \"wb\") as f:\n","        pickle.dump([train_loss_list, test_loss_list, stats_list], f)\n","      with open(tokenizer_path, \"wb\") as f:\n","        pickle.dump(tokenizer, f)\n","\n","      offset += step\n","      #saving offset\n","      with open(offset_path, \"wb\") as f:\n","        pickle.dump(offset, f)\n","        \n","      print(\"Test accuracy\")\n","      evaluate(X_test, Y_test, BATCH_SIZE)\n"],"id":"SVxjjXP5Kou5"},{"cell_type":"code","source":["print(offset)"],"metadata":{"id":"2QYvSpnElXAU"},"id":"2QYvSpnElXAU","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"SfPZQs2TsJ5F"},"id":"SfPZQs2TsJ5F","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"0ijhVe4asYvk"},"id":"0ijhVe4asYvk","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"V5pXoiOcsnZD"},"id":"V5pXoiOcsnZD","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"IaAfJfwWs2Cj"},"id":"IaAfJfwWs2Cj","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"AymgZTj4tEsF"},"id":"AymgZTj4tEsF","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"jcLT0lEwtT00"},"id":"jcLT0lEwtT00","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"prhsJ8_jtieV"},"id":"prhsJ8_jtieV","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RRTooux_XGFL"},"outputs":[],"source":["print(len(test_dataset_label))"],"id":"RRTooux_XGFL"},{"cell_type":"code","execution_count":null,"metadata":{"id":"h7iQ5qbIX3II"},"outputs":[],"source":["print(test_dataset_label)"],"id":"h7iQ5qbIX3II"},{"cell_type":"code","execution_count":null,"metadata":{"id":"15ace80b"},"outputs":[],"source":[""],"id":"15ace80b"},{"cell_type":"code","execution_count":null,"metadata":{"id":"7868c3b0"},"outputs":[],"source":[""],"id":"7868c3b0"},{"cell_type":"code","source":[""],"metadata":{"id":"cSyKH_Octwbj"},"id":"cSyKH_Octwbj","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"695fd084"},"outputs":[],"source":["torch.save(linear_layer.state_dict(), linear_path)\n","torch.save(cross_entropy_layer.state_dict(), cross_path)\n","torch.save(transformer_model.state_dict(), transformer_path)\n","\n","with open(data_path, \"wb\") as f:\n","  pickle.dump([train_loss_list, test_loss_list, stats_list], f)\n","with open(tokenizer_path, \"wb\") as f:\n","  pickle.dump(tokenizer, f)"],"id":"695fd084"},{"cell_type":"code","execution_count":null,"metadata":{"id":"10d3643c"},"outputs":[],"source":["linear_layer.load_state_dict(torch.load(linear_path, map_location=device))\n","cross_entropy_layer.load_state_dict(torch.load(cross_path, map_location=device))\n","transformer_model.load_state_dict(torch.load(transformer_path , map_location=device))\n","with open(data_path, \"rb\") as f:\n","  data = pickle.load(f)\n","  stats_list = data[2]\n","  train_loss_list = data[0]\n","  test_loss_list = data[1]\n","with open(tokenizer_path, \"rb\") as f:\n","  tokenizer = pickle.load(f)"],"id":"10d3643c"},{"cell_type":"code","execution_count":null,"metadata":{"id":"915fcfb4"},"outputs":[],"source":["from multiprocessing import Process, Manager, Array\n","manager = Manager()"],"id":"915fcfb4"},{"cell_type":"code","execution_count":null,"metadata":{"id":"de4a3db0"},"outputs":[],"source":["graph_attention = []\n","for i in range(12):\n","  graph_attention.append({})\n","mapping_ind = {}\n","rev_mapping_ind = {}"],"id":"de4a3db0"},{"cell_type":"code","execution_count":null,"metadata":{"id":"2adb05a4"},"outputs":[],"source":["cnt = 0\n","threshold = 0.01"],"id":"2adb05a4"},{"cell_type":"code","execution_count":null,"metadata":{"id":"46c20049"},"outputs":[],"source":["import multiprocessing as mp\n","from tqdm import tqdm"],"id":"46c20049"},{"cell_type":"code","execution_count":null,"metadata":{"id":"df0a064e"},"outputs":[],"source":["import os\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""],"id":"df0a064e"},{"cell_type":"code","execution_count":null,"metadata":{"id":"636f9d72"},"outputs":[],"source":["\n","\n","my_arr = [1 for i in range(100)]\n","\n","def func(i, j , k):\n","    global my_arr\n","    my_arr[i] = 3\n","    \n","\n","indexes = [(i, i, i) for i in range(12)]\n","pool = mp.Pool(min(12, mp.cpu_count()))\n","pool.starmap(func, indexes)\n","pool.close()"],"id":"636f9d72"},{"cell_type":"code","execution_count":null,"metadata":{"id":"6d09448b"},"outputs":[],"source":["type(mp.cpu_count())"],"id":"6d09448b"},{"cell_type":"code","execution_count":null,"metadata":{"id":"009ef7d6"},"outputs":[],"source":["temp_attention = []\n","def init_arr(arr):\n","    globals()['arr'] = arr\n","from copy import deepcopy\n","def store_attention(lay, j, tokens):\n","        \n","    global mapping_ind, graph_attention, temp_attention;\n","    maximum_attention = np.max(temp_attention)\n","    weights = temp_attention[j, lay, :, :]\n","    temp_dict = {}\n","    for ind_1 in range(len(tokens)):\n","        for ind_2 in range(len(tokens)):\n","            if(tokens[ind_1] in [\"[NEWLINE]\", '[PAD]'] or tokens[ind_2] in [\"[NEWLINE]\", '[PAD]']):\n","                continue\n","            node_1 = mapping_ind[tokens[ind_1]]\n","            node_2 = mapping_ind[tokens[ind_2]]\n","            weight = weights[ind_1, ind_2]\n","\n","            if(weight < threshold*maximum_attention):\n","                continue\n","            if(node_1 not in temp_dict):\n","                temp_dict[node_1] = {}\n","            \n","            if(node_2  not in temp_dict[node_1]):\n","                temp_dict[node_1][node_2] = (weight, 1)\n","            else:\n","                temp_dict[node_1][node_2] = (weight*temp_dict[node_1][node_2][1]/(temp_dict[node_1][node_2][1] + 1), temp_dict[node_1][node_2][1] + 1)\n","    graph_attention[lay] = deepcopy(temp_dict)\n","#     print(graph_attention)\n","def attention_graph(dataset_data, dataset_label, BATCH_SIZE):\n","    \n","    \n","    accumulate_over = 4\n","    global cnt, temp_attention, manager;\n","    optimizer.zero_grad()\n","\n","    for i, (tokenized_threads, labels) in tqdm(enumerate(generator(dataset_data, dataset_label, batch_size = BATCH_SIZE))):\n","    \n","        #Cast to PyTorch tensor\n","        tokenized_threads = torch.tensor(tokenized_threads, device=device)\n","        \n","        pad_mask = torch.where(tokenized_threads!=tokenizer.pad_token_id, 1, 0)\n","    \n","        attention = transformer_model(input_ids=tokenized_threads,\n","                                            attention_mask=pad_mask,)[-1][0].cpu().detach().numpy()\n","        temp_attention = attention[:, :, :, :]\n","        maximum_attention = np.max(attention)\n","        for j, tokenized_thread in enumerate(tokenized_threads):\n","            tokens = tokenizer.convert_ids_to_tokens(tokenized_thread) \n","            # print(len(tokens))\n","            # print(tokenized_thread.shape)\n","            for tok in tokens:\n","                if(tok not in mapping_ind):\n","                    mapping_ind[tok] = cnt;\n","                    cnt += 1\n","            \n","    for i, (tokenized_threads, labels) in tqdm(enumerate(generator(dataset_data, dataset_label, batch_size = BATCH_SIZE))):\n","    \n","        #Cast to PyTorch tensor\n","        tokenized_threads = torch.tensor(tokenized_threads, device=device)\n","        \n","        pad_mask = torch.where(tokenized_threads!=tokenizer.pad_token_id, 1, 0)\n","    \n","        attention = transformer_model(input_ids=tokenized_threads,\n","                                            attention_mask=pad_mask,)[-1][0].cpu().detach().numpy()\n","        temp_attention = attention[:, :, :, :]\n","        maximum_attention = np.max(attention)\n","        for j, tokenized_thread in enumerate(tokenized_threads):\n","            tokens = tokenizer.convert_ids_to_tokens(tokenized_thread) \n","\n","#             params = [(i1, j, tokens) for i1 in range(12)]\n","#             pool = mp.Pool(min(12, mp.cpu_count()))\n","#             pool.starmap(store_attention, params)\n","#             pool.close()\n","            for lay in range(12):\n","                weights = attention[j, lay, :, :]\n","                for ind_1 in range(len(tokens)):\n","                    for ind_2 in range(len(tokens)):\n","                        if(tokens[ind_1] in [\"[NEWLINE]\", '[PAD]'] or tokens[ind_2] in [\"[NEWLINE]\", '[PAD]']):\n","                            continue\n","\n","                        node_1 = mapping_ind[tokens[ind_1]]\n","                        node_2 = mapping_ind[tokens[ind_2]]\n","                        weight = weights[ind_1, ind_2]\n","                        if(weight < threshold*maximum_attention):\n","                            continue\n","                        if(node_1 not in graph_attention[lay]):\n","                            graph_attention[lay][node_1] = {}\n","                        if(node_2  not in graph_attention[lay][node_1]):\n","                            graph_attention[lay][node_1][node_2] = (weight, 1)\n","                        else:\n","                            graph_attention[lay][node_1][node_2] = ((weight + (graph_attention[lay][node_1][node_2][0])*graph_attention[lay][node_1][node_2][1])/(graph_attention[lay][node_1][node_2][1] + 1), graph_attention[lay][node_1][node_2][1] + 1)\n","#         print(cnt, cnt**2)              \n"],"id":"009ef7d6"},{"cell_type":"code","execution_count":null,"metadata":{"id":"d374c29c"},"outputs":[],"source":["def extract_data(dataset_data, dataset_label, BATCH_SIZE):\n","    example = [[], [], []]\n","    for i, (tokenized_threads, labels) in enumerate(generator(dataset_data, dataset_label, batch_size = BATCH_SIZE)):\n","        for j in range(BATCH_SIZE):\n","            if(len(example[0]) < 5 or len(example[1]) < 5 or len(example[2]) < 5):\n","                example[labels[j]].append((tokenizer.convert_ids_to_tokens(tokenized_threads[j]), tokenized_threads[j]))\n","            else:\n","                break\n","    return example"],"id":"d374c29c"},{"cell_type":"code","execution_count":null,"metadata":{"id":"a4001001"},"outputs":[],"source":[""],"id":"a4001001"},{"cell_type":"code","execution_count":null,"metadata":{"id":"574da0e9"},"outputs":[],"source":["text = extract_data(test_dataset_data, test_dataset_label, BATCH_SIZE)"],"id":"574da0e9"},{"cell_type":"code","execution_count":null,"metadata":{"id":"962d9fba"},"outputs":[],"source":["import random"],"id":"962d9fba"},{"cell_type":"code","source":[""],"metadata":{"id":"frRNyL4rhXdL"},"id":"frRNyL4rhXdL","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a1688a04"},"outputs":[],"source":["def find_degree(nodes, graph):\n","    out_degree = {}\n","    in_degree = {}\n","    for node_1 in graph:\n","        for node_2 in graph[node_1]:\n","            if(node_1 not in out_degree):\n","                out_degree[node_1] = 0\n","            if(node_2 not in in_degree):\n","                in_degree[node_2] = 0;\n","            in_degree[node_2] += 1;\n","            out_degree[node_1] += 1;\n","    ret_out = [out_degree[i] for i in nodes]\n","    ret_in = [in_degree[i] for i in nodes]\n","    return ret_out, ret_in\n","def find_topk_tokens(graph, k):\n","    attention = {}\n","    attention_inward = {}\n","    for node_1 in graph:\n","        if(node_1 not in attention):\n","            attention[node_1] = (0, 0)\n","        for node_2 in graph[node_1]:\n","            \n","            if(node_2 not in attention_inward):\n","                attention_inward[node_2] = (0, 0)\n","            attention[node_1] = ((graph[node_1][node_2][0] + (attention[node_1][0])*attention[node_1][1])/(attention[node_1][1] + 1), attention[node_1][1] + 1)\n","            attention_inward[node_2] = ((graph[node_1][node_2][0] + (attention_inward[node_2][0])*attention_inward[node_2][1])/(attention_inward[node_2][1] + 1), attention_inward[node_2][1] + 1)\n","    out_max = [i[0] for i in find_maxk(attention, k)]\n","    in_max = [i[0] for i in find_maxk(attention_inward, k)]\n","    return in_max, out_max, [i[1] for i in find_maxk(attention_inward, k)], [i[1] for i in find_maxk(attention, k)], find_mink(attention, k)[0][1], find_mink(attention_inward, k)[0][1]\n","def find_mink(attention, k):\n","    ret = []\n","    for i in attention:\n","        if(i == mapping_ind[\"[CLS]\"] or i == mapping_ind[\"[SEP]\"]):\n","            continue\n","        ret.append((i, attention[i][0]))\n","    ret.sort(key = lambda x: x[1])\n","    return ret[:k]\n","def find_maxk(attention, k):\n","    ret = []\n","    for i in attention:\n","        if(i == mapping_ind[\"[CLS]\"] or i == mapping_ind[\"[SEP]\"]):\n","            continue\n","        ret.append((i, attention[i][0]))\n","    ret.sort(key = lambda x: x[1])\n","    ret.reverse()\n","    # print(ret)\n","    return ret[:k]"],"id":"a1688a04"},{"cell_type":"code","execution_count":null,"metadata":{"id":"38b39577"},"outputs":[],"source":["def remove_pad(given_s):\n","    l = given_s.split()\n","    ret = []\n","    for i in l:\n","        if(i != \"[PAD]\"):\n","            ret.append(i)\n","    return \" \".join(ret)"],"id":"38b39577"},{"cell_type":"code","execution_count":null,"metadata":{"id":"9521268e"},"outputs":[],"source":["def replacing(tok_mapping, tokenized_thread, tokens):\n","    ret_tokenized_thread = tokenized_thread.clone()\n","    ind = []\n","    for i in range(len(tokens)):\n","        if(mapping_ind[tokens[i]] in tok_mapping):\n","            ret_tokenized_thread[0, i] = tokenizer.encode(tok_mapping[mapping_ind[tokens[i]]])[1];\n","            ind.append(i)\n","    return ret_tokenized_thread, ind"],"id":"9521268e"},{"cell_type":"code","source":["vocab_dict = tokenizer.get_vocab()"],"metadata":{"id":"cbu6A4GRmwGr"},"id":"cbu6A4GRmwGr","execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","vocab = []\n","for i in vocab_dict:\n","  if(i not in [\"[CLS]\", '[PAD]', '[SEP]']):\n","    vocab.append(i)\n","\n"],"metadata":{"id":"4mTGYoz4mzVr"},"id":"4mTGYoz4mzVr","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ede1c1f4"},"outputs":[],"source":["import random\n","def create_mapping(tok, ind):\n","    ret_dict = {}\n","    for k, i in enumerate(tok):\n","        if((k + i) < len(vocab)):\n","            ret_dict[i] = vocab[k + ind]\n","        else:\n","            ret_dict[i] = random.choice(vocab)\n","        ret_dict[i] = random.choice(vocab)\n","    return ret_dict"],"id":"ede1c1f4"},{"cell_type":"code","execution_count":null,"metadata":{"id":"8ac4f636"},"outputs":[],"source":["def get_embedding(ind, tokenized):\n","    pad_mask = torch.where(tokenized!=tokenizer.pad_token_id, 1, 0)\n","    embedding = transformer_model(input_ids=tokenized,attention_mask=pad_mask,).last_hidden_state\n","    ret_embedding = []\n","    for i in ind:\n","        ret_embedding.append(embedding[:, i, :].reshape((embedding.shape[2])))\n","    return ret_embedding"],"id":"8ac4f636"},{"cell_type":"code","execution_count":null,"metadata":{"id":"69b6a039"},"outputs":[],"source":["replacement = 100\n","trials = 3\n","trial_candidate = 5\n","select_max = 5"],"id":"69b6a039"},{"cell_type":"code","execution_count":null,"metadata":{"id":"d5443e43"},"outputs":[],"source":["print(len(text[1]))"],"id":"d5443e43"},{"cell_type":"code","execution_count":null,"metadata":{"id":"de3815a1"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from scipy import spatial\n","text[0] = text[0][:3]\n","text[1] = text[1][:3]\n","text[2] = text[2][:3]"],"id":"de3815a1"},{"cell_type":"code","execution_count":null,"metadata":{"id":"d1438b66"},"outputs":[],"source":["def predict_proba(tokenized_threads):\n","    pad_mask = torch.where(tokenized_threads!=tokenizer.pad_token_id, 1, 0)\n","    logits = linear_layer(transformer_model(input_ids=tokenized_threads,attention_mask=pad_mask,).last_hidden_state[:, 0, :])\n","    logits = torch.exp(logits)\n","    z = torch.sum(logits)\n","    logits = torch.divide(logits, z)\n","    return logits\n","def mask_out_top5(text_lab, label, label_ind, flag):\n","    accumulate_over = 4\n","    global cnt, temp_attention, manager;\n","    optimizer.zero_grad()\n","\n","    for i1, (tokens, tokenized_threads) in enumerate(text_lab):\n","        print(\"\\n\\n\\n\")\n","        print(\"###### Example :\", str(i1), \"label\", label, \"########\")\n","        #Cast to PyTorch tensor\n","        tokenized_threads = torch.tensor(tokenized_threads, device=device).reshape((1, len(tokenized_threads)))\n","        \n","        pad_mask = torch.where(tokenized_threads!=tokenizer.pad_token_id, 1, 0)\n","        \n","        attention = transformer_model(input_ids=tokenized_threads,\n","                                            attention_mask=pad_mask,)[-1][0].cpu().detach().numpy()\n","        graph = dict()\n","        for tok in tokens:\n","            if(tok not in mapping_ind):\n","                mapping_ind[tok] = cnt;\n","                cnt += 1\n","\n","        for lay in range(12):\n","            weights = attention[0, lay, :, :]\n","\n","            for ind_1 in range(len(tokens)):\n","                for ind_2 in range(len(tokens)):\n","                    if(tokens[ind_1] in [\"[NEWLINE]\", '[PAD]'] or tokens[ind_2] in [\"[NEWLINE]\", '[PAD]']):\n","                        continue\n","                    node_1 = mapping_ind[tokens[ind_1]]\n","                    node_2 = mapping_ind[tokens[ind_2]]\n","                    rev_mapping_ind[node_1] = tokens[ind_1]\n","                    rev_mapping_ind[node_2] = tokens[ind_2]\n","                    weight = weights[ind_1, ind_2]\n","                    if(node_1 not in graph):\n","                        graph[node_1] = {}\n","                    if(node_2  not in graph[node_1]):\n","                        graph[node_1][node_2] = (weight, 1)\n","                    else:\n","                        graph[node_1][node_2] = ((weight + (graph[node_1][node_2][0])*graph[node_1][node_2][1])/(graph[node_1][node_2][1] + 1), graph[node_1][node_2][1] + 1)\n","\n","        topk_in, topk_out, topk_in_attention, topk_out_attention, topk_in_min, topk_out_min  = find_topk_tokens(graph, select_max)\n","\n","        \n","        candidate = random.sample(topk_in, trial_candidate)\n","        for j in range(trials):\n","            random.shuffle(vocab)\n","            print([ rev_mapping_ind[z] for z in candidate], \"\\n\")\n","            in_logits_l2_norm = []\n","            for l in tqdm(range(0, min(len(vocab), 1000), select_max)):\n","                top5_in_mapping = create_mapping(candidate, l)\n","\n","                example_in, ind_in = replacing(top5_in_mapping, tokenized_threads, tokens)\n","\n","                actual_in = get_embedding(ind_in, tokenized_threads)\n","                replaced_in = get_embedding(ind_in, example_in)\n","\n","                if(len(ind_in) >= 1):\n","                    final_actual_in = torch.divide(sum(actual_in),len(actual_in)).cpu().detach().numpy()\n","                    final_replaced_in = torch.divide(sum(replaced_in) , len(replaced_in)).cpu().detach().numpy()\n","                else:\n","                    continue\n","                if(predict_proba(example_in).cpu().detach().numpy().tolist()[0][label_ind] < 0.5):\n","                    continue\n","                subtract_in = np.subtract(final_actual_in, final_replaced_in)\n","                if(flag == \"L2\"):\n","                    in_logits_l2_norm.append((np.linalg.norm(subtract_in), predict_proba(example_in).cpu().detach().numpy().tolist()[0][label_ind]))\n","                elif(flag == \"Cosine\"):\n","                    in_logits_l2_norm.append((spatial.distance.cosine(final_actual_in, final_replaced_in), predict_proba(example_in).cpu().detach().numpy().tolist()[0][label_ind]))\n","                    \n","                    \n","            in_logits_l2_norm.sort(key = lambda x : x[0])\n","            \n","\n","            plt.scatter([i[0] for i in in_logits_l2_norm],[i[1] for i in in_logits_l2_norm])\n","        print(\"| Min:\", topk_in_min)\n","        for i in range(len(topk_in)):\n","            print(\"Token:\", rev_mapping_ind[topk_in[i]], \"| attention:\", topk_in_attention[i])\n","        plt.title('Example ' + str(i) + ' in')\n","        plt.xlabel(flag + ' norm')\n","        plt.ylabel('probability of '+ label )\n","        plt.show()\n","        plt.clf()\n","        \n","        candidate = random.sample(topk_out, trial_candidate)\n","        for j in range(trials):\n","\n","            random.shuffle(vocab)\n","            print([ rev_mapping_ind[z] for z in candidate], \"\\n\")\n","            out_logits_l2_norm = []\n","            for l in tqdm(range(0, min(len(vocab), 1000), select_max)):\n","                top5_out_mapping = create_mapping([j], l)\n","\n","                \n","                \n","                example_out, ind_out = replacing(top5_out_mapping, tokenized_threads, tokens)\n","\n","                actual_out = get_embedding(ind_out, tokenized_threads)\n","                replaced_out = get_embedding(ind_out, example_out)\n","#                 print(sum(actual_out))\n","                if(len(ind_out) >= 1):\n","                    final_actual_out = torch.divide(sum(actual_out) , len(actual_out)).cpu().detach().numpy()\n","                    final_replaced_out = torch.divide(sum(replaced_out) , len(replaced_out)).cpu().detach().numpy()\n","                else:\n","                    continue\n","                if(predict_proba(example_out).cpu().detach().numpy().tolist()[0][label_ind] < 0.50):\n","                    continue    \n","                subtract_out = np.subtract(final_actual_out, final_replaced_out)\n","                if(flag == \"L2\"):\n","                    out_logits_l2_norm.append((np.linalg.norm(subtract_out), predict_proba(example_out).cpu().detach().numpy().tolist()[0][label_ind]))\n","                elif(flag == \"Cosine\"):\n","                    out_logits_l2_norm.append((spatial.distance.cosine(final_actual_out, final_replaced_out), predict_proba(example_out).cpu().detach().numpy().tolist()[0][label_ind]))\n","                    \n","            out_logits_l2_norm.sort(key = lambda x : x[0])\n","            \n","            plt.scatter([i[0] for i in out_logits_l2_norm],[i[1] for i in out_logits_l2_norm])\n","\n","        print(\"| Min:\", topk_out_min)\n","        for i in range(len(topk_out)):\n","            print(\"Token:\", rev_mapping_ind[topk_out[i]], \"| attention:\", topk_out_attention[i])\n","        plt.title('Example ' + str(i) + ' out')\n","        plt.xlabel(flag + ' norm')\n","        plt.ylabel('probability of ' + label )\n","        plt.show()\n","        plt.clf()"],"id":"d1438b66"},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ba492dd","scrolled":false},"outputs":[],"source":["\n","mask_out_top5(text[0], \"neutral\", 0, \"L2\")"],"id":"5ba492dd"},{"cell_type":"code","execution_count":null,"metadata":{"id":"bd2db3d9","scrolled":false},"outputs":[],"source":["mask_out_top5(text[1], \"entailment\", 1, \"L2\",)"],"id":"bd2db3d9"},{"cell_type":"code","source":["mask_out_top5(text[2], \"contradiction\", 2, \"L2\",)"],"metadata":{"id":"OxRpm1ta4vzE"},"id":"OxRpm1ta4vzE","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6632f9b8","scrolled":false},"outputs":[],"source":["mask_out_top5(text[0], \"neutral\", 0, \"Cosine\")"],"id":"6632f9b8"},{"cell_type":"code","execution_count":null,"metadata":{"id":"39ff9905","scrolled":false},"outputs":[],"source":["mask_out_top5(text[1], \"entailment\", 1, \"Cosine\",)"],"id":"39ff9905"},{"cell_type":"code","source":["mask_out_top5(text[2], \"contradiction\", 2, \"Cosine\",)"],"metadata":{"id":"rXd1ACZ94zEI"},"id":"rXd1ACZ94zEI","execution_count":null,"outputs":[]},{"cell_type":"code","source":["select_max_imp = 100"],"metadata":{"id":"ZZZg4KcY27NR"},"id":"ZZZg4KcY27NR","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_imp(true_logits, true_class, perturbed_logits, perturbed_class):\n","  if(perturbed_class == true_class):\n","    return true_logits[true_class] - perturbed_logits[true_class]\n","  return true_logits[true_class] - perturbed_logits[true_class] + true_logits[perturbed_class] - perturbed_logits[perturbed_class]"],"metadata":{"id":"1B6M0GM02_d6"},"id":"1B6M0GM02_d6","execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(tokenizer.decode([1532]))"],"metadata":{"id":"sFNM8dQo4GCE"},"id":"sFNM8dQo4GCE","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def intersection(a, b):\n","  cnt = 0;\n","  print(a)\n","  print(b)\n","  for i in range(min(len(a), len(b))):\n","      if(a[i] == b[i]):\n","          cnt += 1;\n","      \n","  return (cnt/min(len(a), len(b)))*100"],"metadata":{"id":"tDtxMSXu3fl9"},"id":"tDtxMSXu3fl9","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"873a9e1b"},"outputs":[],"source":["def find_intersection_top5(text_lab, label, label_ind, flag):\n","    accumulate_over = 4\n","    global cnt, temp_attention, manager;\n","    optimizer.zero_grad()\n","\n","    for i, (tokens, tokenized_threads) in enumerate(text_lab):\n","        print(\"\\n\\n\\n\")\n","        print(\"###### Example :\", str(i), \"label\", label, \"########\")\n","        #Cast to PyTorch tensor\n","        tokenized_threads = torch.tensor(tokenized_threads, device=device).reshape((1, len(tokenized_threads)))\n","        \n","        pad_mask = torch.where(tokenized_threads!=tokenizer.pad_token_id, 1, 0)\n","        \n","        attention = transformer_model(input_ids=tokenized_threads,\n","                                            attention_mask=pad_mask,)[-1][0].cpu().detach().numpy()\n","        graph = dict()\n","        \n","        for tok in tokens:\n","            if(tok not in mapping_ind):\n","                mapping_ind[tok] = cnt;\n","                cnt += 1\n","\n","        for lay in range(12):\n","            weights = attention[0, lay, :, :]\n","            for ind_1 in range(len(tokens)):\n","                for ind_2 in range(len(tokens)):\n","                    if(tokens[ind_1] in [\"[NEWLINE]\", '[PAD]'] or tokens[ind_2] in [\"[NEWLINE]\", '[PAD]']):\n","                        continue\n","                    node_1 = mapping_ind[tokens[ind_1]]\n","                    node_2 = mapping_ind[tokens[ind_2]]\n","                    rev_mapping_ind[node_1] = tokens[ind_1]\n","                    rev_mapping_ind[node_2] = tokens[ind_2]\n","                    weight = weights[ind_1, ind_2]\n","                    if(node_1 not in graph):\n","                        graph[node_1] = {}\n","                    if(node_2  not in graph[node_1]):\n","                        graph[node_1][node_2] = (weight, 1)\n","                    else:\n","                        graph[node_1][node_2] = ((weight + (graph[node_1][node_2][0])*graph[node_1][node_2][1])/(graph[node_1][node_2][1] + 1), graph[node_1][node_2][1] + 1)\n","        logits = predict_proba(tokenized_threads)\n","        class_assigned = torch.argmax(logits[0, :])\n","        topk_in, topk_out, topk_in_attention, topk_out_attention, topk_in_min, topk_out_min = find_topk_tokens(graph, select_max_imp)\n","        arr = tokenized_threads.cpu().detach().clone().numpy()\n","\n","        candidates = []\n","        for j in range(len(tokens)):\n","          temp_tokenized_threads = np.concatenate([arr[0, : j], arr[0, j + 1:], np.array([tokenizer.pad_token_id])])\n","    \n","          temp_logits = predict_proba(torch.tensor(temp_tokenized_threads, device=device).reshape((1, len(temp_tokenized_threads))))\n","\n","          temp_class_assigned = torch.argmax(temp_logits)\n","\n","          importance_score = calculate_imp(logits[0], class_assigned, temp_logits[0], temp_class_assigned)\n","          candidates.append((arr[0, j], importance_score))\n","        candidates.sort(key = lambda x : x[1])\n","        candidates.reverse()\n","        print(candidates)\n","        print(\"intersection between in : \", intersection([tokenizer.decode(i[0]) for i in candidates[:select_max_imp] if tokenizer.decode(i[0]) not in [\"[CLS]\", \"[PAD]\", \"[SEP]\"]], [rev_mapping_ind[z] for z in topk_in]))\n","        print(\"intersection between out : \",intersection([tokenizer.decode(i[0]) for i in candidates[:select_max_imp] if tokenizer.decode(i[0]) not in [\"[CLS]\", \"[PAD]\",\"[SEP]\"]], [rev_mapping_ind[z] for z in topk_out]))"],"id":"873a9e1b"},{"cell_type":"code","source":[""],"metadata":{"id":"GGFYzCD8BNwF"},"id":"GGFYzCD8BNwF","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ef61e42f"},"outputs":[],"source":["\n","find_intersection_top5(text[2], \"contradiction\", 2, \"Cosine\",)"],"id":"ef61e42f"},{"cell_type":"code","source":["find_intersection_top5(text[1], \"entailment\", 1, \"L2\",)"],"metadata":{"id":"juLER5nv88I6"},"id":"juLER5nv88I6","execution_count":null,"outputs":[]},{"cell_type":"code","source":["find_intersection_top5(text[0], \"neutral\", 0, \"L2\")"],"metadata":{"id":"i-hpujuO884I"},"id":"i-hpujuO884I","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"lJFYLps5v09d"},"id":"lJFYLps5v09d","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"NW0vLDFybMHf"},"id":"NW0vLDFybMHf","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"FOV-2In7baw7"},"id":"FOV-2In7baw7","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"lHYf7jDgbpad"},"id":"lHYf7jDgbpad","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"ux_WV4TPb35N"},"id":"ux_WV4TPb35N","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"jMjf1vKSckAz"},"id":"jMjf1vKSckAz","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"u6C9zJxZcyp5"},"id":"u6C9zJxZcyp5","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"-MkESACidBTg"},"id":"-MkESACidBTg","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"7ZdfrDcMdP86"},"id":"7ZdfrDcMdP86","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"gRjVE9D0hmR1"},"id":"gRjVE9D0hmR1","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"GTadbkeYh07C"},"id":"GTadbkeYh07C","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"EZ1kfg5tiDku"},"id":"EZ1kfg5tiDku","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"opUIWCSeiSOC"},"id":"opUIWCSeiSOC","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"aERRzHvQig3j"},"id":"aERRzHvQig3j","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"64gnkKKLivhN"},"id":"64gnkKKLivhN","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"LjFUaILui-Kj"},"id":"LjFUaILui-Kj","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Z3w3VtTkkDvi"},"id":"Z3w3VtTkkDvi","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"70JEeqjak-Vi"},"id":"70JEeqjak-Vi","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"OKZBB46wl47i"},"id":"OKZBB46wl47i","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"uNMLCpfZmGRI"},"id":"uNMLCpfZmGRI","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"YTBLL4k8mLdv"},"id":"YTBLL4k8mLdv","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"rmKrCQhUmaca"},"id":"rmKrCQhUmaca","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"_8437tpumpg5"},"id":"_8437tpumpg5","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"jp05Bbb0m3bj"},"id":"jp05Bbb0m3bj","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"rgq6LBIrnGUp"},"id":"rgq6LBIrnGUp","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"ZSPQpNoOn9Ap"},"id":"ZSPQpNoOn9Ap","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"wShf8I_qopM7"},"id":"wShf8I_qopM7","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"90Ilfm7vBS9S"},"id":"90Ilfm7vBS9S","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"M9C_JqtvBhm4"},"id":"M9C_JqtvBhm4","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"ENVfSYlDBwQq"},"id":"ENVfSYlDBwQq","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"dC4V7C30B-5u"},"id":"dC4V7C30B-5u","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"0HjJdsUPCNjO"},"id":"0HjJdsUPCNjO","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"UAEgAlkRCeZo"},"id":"UAEgAlkRCeZo","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"UEVJvvUwCtDK"},"id":"UEVJvvUwCtDK","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"uNpRcSCZC7sp"},"id":"uNpRcSCZC7sp","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"TugCHJ2FDKWL"},"id":"TugCHJ2FDKWL","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"S4FVfgaIDY_z"},"id":"S4FVfgaIDY_z","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"UUOJbPBKDnpK"},"id":"UUOJbPBKDnpK","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"UTMgsRNAD2Ss"},"id":"UTMgsRNAD2Ss","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"pwjIqRDGEE8F"},"id":"pwjIqRDGEE8F","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"JPdHynkmETlo"},"id":"JPdHynkmETlo","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Qe_-_jTOEiPO"},"id":"Qe_-_jTOEiPO","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"AicXy7RiEw4t"},"id":"AicXy7RiEw4t","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"OvTgrtcZE_iN"},"id":"OvTgrtcZE_iN","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"PSLdhoaMFOLt"},"id":"PSLdhoaMFOLt","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"SVCNWQnmFc1R"},"id":"SVCNWQnmFc1R","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"sZPUZ3x5Frem"},"id":"sZPUZ3x5Frem","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"jSuWuQ0VF6IN"},"id":"jSuWuQ0VF6IN","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"nJCczd6fGIyw"},"id":"nJCczd6fGIyw","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"mwDISiDFGWVK"},"id":"mwDISiDFGWVK","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"K4NWf8EdGj3w"},"id":"K4NWf8EdGj3w","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"rbyhNWgnGyhS"},"id":"rbyhNWgnGyhS","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"ZRKS00fHHBKv"},"id":"ZRKS00fHHBKv","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"pmlhInZdHQD6"},"id":"pmlhInZdHQD6","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"je8Nd6x2Hetc"},"id":"je8Nd6x2Hetc","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"VqYeJzk7IbQm"},"id":"VqYeJzk7IbQm","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"oWDd0WACJHNM"},"id":"oWDd0WACJHNM","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"fE9uxHBrJkgL"},"id":"fE9uxHBrJkgL","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"-cXrDGz6KkfD"},"id":"-cXrDGz6KkfD","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"fy5I6zXlKryV"},"id":"fy5I6zXlKryV","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"TylePPNYK6rg"},"id":"TylePPNYK6rg","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"TniGHGJVLJU_"},"id":"TniGHGJVLJU_","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"0xHudDOiLYOE"},"id":"0xHudDOiLYOE","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"8mD-EQtELmn-"},"id":"8mD-EQtELmn-","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"nWrXcGtEMFoq"},"id":"nWrXcGtEMFoq","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"UlhpHpcSMi8Q"},"id":"UlhpHpcSMi8Q","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Pp8vXnD1Mxlb"},"id":"Pp8vXnD1Mxlb","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"JezwovWZNAOz"},"id":"JezwovWZNAOz","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Ux8Lfmw9NO4M"},"id":"Ux8Lfmw9NO4M","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"VndZqhnrNdhz"},"id":"VndZqhnrNdhz","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"g4lANuyQNsLW"},"id":"g4lANuyQNsLW","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"olPC3uN_N607"},"id":"olPC3uN_N607","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"P2iUfTycOJeG"},"id":"P2iUfTycOJeG","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"WiN6zJNYOYHl"},"id":"WiN6zJNYOYHl","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"HNnVV7LsOmxe"},"id":"HNnVV7LsOmxe","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"2-4B865jO1ax"},"id":"2-4B865jO1ax","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"yZN9ZQsCPEEX"},"id":"yZN9ZQsCPEEX","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"U4FzSYX0PSt-"},"id":"U4FzSYX0PSt-","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"REDqysp-PhXS"},"id":"REDqysp-PhXS","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"i8usvcQwPwA1"},"id":"i8usvcQwPwA1","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"SZEO0CbXP-qX"},"id":"SZEO0CbXP-qX","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"RHV-F3GMQNT8"},"id":"RHV-F3GMQNT8","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"vFDTfIl8Qb9V"},"id":"vFDTfIl8Qb9V","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"k8YNFu7LQqmw"},"id":"k8YNFu7LQqmw","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"82t07PYvQ5QN"},"id":"82t07PYvQ5QN","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"6bjl9SxJRH52"},"id":"6bjl9SxJRH52","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"ixYByf5jRWjS"},"id":"ixYByf5jRWjS","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"JAMH3w0URlM1"},"id":"JAMH3w0URlM1","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"hqKZUGYbRz2X"},"id":"hqKZUGYbRz2X","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"gtXZqvOXSCf3"},"id":"gtXZqvOXSCf3","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"PSYKapkMSRJI"},"id":"PSYKapkMSRJI","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"yBXHvB35Sfyp"},"id":"yBXHvB35Sfyp","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"nMoqNdefSucT"},"id":"nMoqNdefSucT","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"k5O5gVrZS9F1"},"id":"k5O5gVrZS9F1","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Aa32d422TLvh"},"id":"Aa32d422TLvh","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"3M-IybjNTaY3"},"id":"3M-IybjNTaY3","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"44jiMS6GTpCS"},"id":"44jiMS6GTpCS","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"20Z-XKw9T3r3"},"id":"20Z-XKw9T3r3","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"rcCkfdRRUGVI"},"id":"rcCkfdRRUGVI","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Q0X9GwCYUU-3"},"id":"Q0X9GwCYUU-3","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"WESGoy5FUjoS"},"id":"WESGoy5FUjoS","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"o4Rg_-aZUwEy"},"id":"o4Rg_-aZUwEy","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"ulL-rtotU-ua"},"id":"ulL-rtotU-ua","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"yqo0JLd3demf"},"id":"yqo0JLd3demf","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"H_sS0JesdtQA"},"id":"H_sS0JesdtQA","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"bvlQJavBeG4k"},"id":"bvlQJavBeG4k","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"FkUhHaIKeViF"},"id":"FkUhHaIKeViF","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"SzudFlGJegwy"},"id":"SzudFlGJegwy","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"7nBcnWIbey1R"},"id":"7nBcnWIbey1R","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"0wCKcTMZfBeu"},"id":"0wCKcTMZfBeu","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"GFZrIA4WfQIM"},"id":"GFZrIA4WfQIM","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"JAou0nasfex1"},"id":"JAou0nasfex1","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"U_nGG9s9ftbQ"},"id":"U_nGG9s9ftbQ","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"XQwhJcLRf8Eq"},"id":"XQwhJcLRf8Eq","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"P_1IixvKgKuP"},"id":"P_1IixvKgKuP","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"nlbjex5ngZXv"},"id":"nlbjex5ngZXv","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"GvspR0W9goBc"},"id":"GvspR0W9goBc","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"ZbABg8ypg2qt"},"id":"ZbABg8ypg2qt","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"TfSP4tpLhFUM"},"id":"TfSP4tpLhFUM","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"flQjx7uPhKpb"},"id":"flQjx7uPhKpb","execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Tensorflow_SNLI_longformer-l2norm-WindowSize-256.ipynb","provenance":[{"file_id":"1R32O37VcXSd8v0GC7aGjcF3jmjqYFg7c","timestamp":1646310169640},{"file_id":"1cFanFD0wTlmAu2HL2i75KJ7MGbh9J6jF","timestamp":1646291363110},{"file_id":"1d2MshebvNIsYetX9IZ9-YtNMCbGD0JYn","timestamp":1646286749562},{"file_id":"1KybEr7xSIS-xNEccotPjR9hSIaVTTf4M","timestamp":1644355651702},{"file_id":"1inCMMEN8anpNDKBiOSWluu4VuQUFOAS4","timestamp":1644352583543},{"file_id":"1BnujenSQAxjEWNLCK31LbnyZwZ_ltG_4","timestamp":1643035219868}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}