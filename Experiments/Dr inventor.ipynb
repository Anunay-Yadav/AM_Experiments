{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Dr inventor.ipynb","provenance":[],"authorship_tag":"ABX9TyPy+StfReIMgXYLv09iV4ic"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"HQSAkQc7liNL"},"source":["global_flag = 4\n","\"\"\"#### Function to get train, test data (50/50 split currently)\"\"\"\n","\n","def get_datasets(train_sz=100, test_sz=0):\n","    train_dataset, valid_dataset, test_dataset = load_dataset(tokenizer=tokenizer,\n","                                                              max_len=4096-337-12,\n","                                                              train_sz=train_sz,\n","                                                              test_sz=test_sz,\n","                                                              shuffle=True,)\n","    return train_dataset, valid_dataset, test_dataset\n","\n","\"\"\"### Define linear layer for a relation type prediction\"\"\"\n","\n","def get_rel_head():\n","    linear_layer = nn.Linear(num_mask_pos*transformer_model.config.hidden_size,\n","                             len(rel_type_dict)).to(device)\n","\n","    return linear_layer\n","\n","\"\"\"### Global Attention Mask Utility for Longformer\"\"\"\n","\n","def get_global_attention_mask(tokenized_threads: np.ndarray) -> np.ndarray:\n","    \"\"\"Returns an attention mask, with 1 where there are [USER{i}] tokens and \n","    0 elsewhere.\n","    \"\"\"\n","    mask = np.zeros_like(tokenized_threads)\n","    for token_id in [tokenizer.sep_token_id, tokenizer.cls_token_id,\n","                     tokenizer.bos_token_id, tokenizer.eos_token_id,]:\n","        mask = np.where(tokenized_threads==token_id, 1, mask)\n","    return np.array(mask, dtype=bool)\n","\n","def get_spans(comp_type_labels, length):\n","    \n","    def detect_span(start_idx: int, span_t: str):\n","        j = start_idx\n","        while j<length and comp_type_labels[j]==ac_dict[span_t]:\n","            j += 1\n","        end_idx = j\n","        return start_idx, end_idx\n","    \n","    i=0\n","    spans_lis = []\n","    while i<length:\n","        if comp_type_labels[i]==ac_dict[\"O\"]:\n","            _start_idx, end_idx = detect_span(i, \"O\")\n","\n","        elif comp_type_labels[i]==ac_dict[\"B-BC\"]:\n","            _start_idx, end_idx = detect_span(i+1, \"I-BC\")\n","            spans_lis.append((i, end_idx))\n","            \n","        elif comp_type_labels[i]==ac_dict[\"B-OC\"]:\n","            _start_idx, end_idx = detect_span(i+1, \"I-OC\")\n","            spans_lis.append((i, end_idx))\n","        \n","        elif comp_type_labels[i]==ac_dict[\"B-D\"]:\n","            _start_idx, end_idx = detect_span(i+1, \"I-D\")\n","            spans_lis.append((i, end_idx))\n","        \n","        elif (comp_type_labels[i]==ac_dict[\"I-BC\"] or\n","              comp_type_labels[i]==ac_dict[\"I-OC\"] or \n","              comp_type_labels[i]==ac_dict[\"I-D\"]):\n","            raise AssertionError(\"Span detection not working properly, \\\n","                                  Or intermediate tokens without begin tokens in\",\n","                                 comp_type_labels)\n","        else:\n","            raise ValueError(\"Unknown component type:\", comp_type_labels[i], \n","                             \"Known types are:\", ac_dict)\n","        \n","        i = end_idx\n","    \n","    return spans_lis\n","def generate_masked_prompts_remove_sent(tokenized_thread, length, from_end_idx, from_start_idx, to_start_idx, to_end_idx, rel_type):\n","    prompt_1 = np.concatenate([np.array([tokenizer.mask_token_id]*length),\n","                                np.array(tokenizer.encode(\"Explaination: We said: \\\"\"))[1:-1],\n","                                tokenized_thread[from_start_idx:from_end_idx],\n","                                np.array(tokenizer.encode(\"\\\"\"))[1:-1],\n","                                np.array([tokenizer.mask_token_id]*num_mask_pos),\n","                                np.array(tokenizer.encode(\"\\\"\"))[1:-1],\n","                                tokenized_thread[to_start_idx:to_end_idx],\n","                                np.array(tokenizer.encode(\"\\\"\"))[1:-1],])\n","    prompt_2 = np.concatenate([tokenized_thread[:length],\n","                                 np.array(tokenizer.encode(\"Explaination: We said: \\\"\"))[1:-1],\n","                                 tokenized_thread[from_start_idx:from_end_idx],\n","                                 np.array(tokenizer.encode(\"\\\"\"))[1:-1],\n","                                 np.array([tokenizer.mask_token_id]*num_mask_pos),\n","                                 np.array(tokenizer.encode(\"\\\"\"))[1:-1],\n","                                 tokenized_thread[to_start_idx:to_end_idx],\n","                                 np.array(tokenizer.encode(\"\\\"\"))[1:-1],])\n","    yield (prompt_1, int(rel_type))\n","    yield (prompt_2, int(rel_type))\n","  \n","def generate_masked_prompts_part_1(tokenized_thread, length, from_end_idx, from_start_idx, to_start_idx, to_end_idx, rel_type):\n","    prompt_1 = np.concatenate([tokenized_thread[:length],\n","                                np.array(tokenizer.encode(\"Explaination: We said: \\\"\"))[1:-1],\n","                                np.array([tokenizer.mask_token_id]*(from_end_idx - from_start_idx)),\n","                                np.array(tokenizer.encode(\"\\\"\"))[1:-1],\n","                                np.array([tokenizer.mask_token_id]*num_mask_pos),\n","                                np.array(tokenizer.encode(\"\\\"\"))[1:-1],\n","                                tokenized_thread[to_start_idx:to_end_idx],\n","                                np.array(tokenizer.encode(\"\\\"\"))[1:-1],])\n","    prompt_2 = np.concatenate([tokenized_thread[:length],\n","                                 np.array(tokenizer.encode(\"Explaination: We said: \\\"\"))[1:-1],\n","                                 tokenized_thread[from_start_idx:from_end_idx],\n","                                 np.array(tokenizer.encode(\"\\\"\"))[1:-1],\n","                                 np.array([tokenizer.mask_token_id]*num_mask_pos),\n","                                 np.array(tokenizer.encode(\"\\\"\"))[1:-1],\n","                                 tokenized_thread[to_start_idx:to_end_idx],\n","                                 np.array(tokenizer.encode(\"\\\"\"))[1:-1],])\n","    yield (prompt_1, int(rel_type))\n","    yield (prompt_2, int(rel_type))\n","\n","def generate_masked_prompts_part_2(tokenized_thread, length, from_end_idx, from_start_idx, to_start_idx, to_end_idx, rel_type):\n","    prompt_1 = np.concatenate([tokenized_thread[:length],\n","                                np.array(tokenizer.encode(\"Explaination: We said: \\\"\"))[1:-1],\n","                                tokenized_thread[from_start_idx:from_end_idx],\n","                                np.array(tokenizer.encode(\"\\\"\"))[1:-1],\n","                                np.array([tokenizer.mask_token_id]*num_mask_pos),\n","                                np.array(tokenizer.encode(\"\\\"\"))[1:-1],\n","                                tokenized_thread[to_start_idx:to_end_idx],\n","                                np.array(tokenizer.encode(\"\\\"\"))[1:-1],])\n","    prompt_2 = np.concatenate([tokenized_thread[:length],\n","                                 np.array(tokenizer.encode(\"Explaination: We said: \\\"\"))[1:-1],\n","                                 tokenized_thread[from_start_idx:from_end_idx],\n","                                 np.array(tokenizer.encode(\"\\\"\"))[1:-1],\n","                                 np.array([tokenizer.mask_token_id]*num_mask_pos),\n","                                 np.array(tokenizer.encode(\"\\\"\"))[1:-1],\n","                                np.array([tokenizer.mask_token_id]*(to_end_idx - to_start_idx)),\n","                                 np.array(tokenizer.encode(\"\\\"\"))[1:-1],])\n","    yield (prompt_1, int(rel_type))\n","    yield (prompt_2, int(rel_type))\n"," \n","def combine_masked_generator(flag, tokenized_thread, length, from_end_idx, from_start_idx, to_start_idx, to_end_idx, rel_type):\n","    if(flag == 1):\n","        for prompt, rel in generate_masked_prompts_remove_sent(tokenized_thread, length, from_end_idx, from_start_idx, to_start_idx, to_end_idx, rel_type):\n","            yield (prompt, int(rel))\n","    elif(flag == 2):\n","        for prompt, rel in generate_masked_prompts_part_1(tokenized_thread, length, from_end_idx, from_start_idx, to_start_idx, to_end_idx, rel_type):\n","            yield (prompt, int(rel))\n","    elif(flag == 3):\n","        for prompt, rel in generate_masked_prompts_part_2(tokenized_thread, length, from_end_idx, from_start_idx, to_start_idx, to_end_idx, rel_type):\n","            yield (prompt, int(rel))\n","    else:\n","        prompt = np.concatenate([tokenized_thread[:length],\n","                                 np.array(tokenizer.encode(\"Explaination: We said: \\\"\"))[1:-1],\n","                                 tokenized_thread[from_start_idx:from_end_idx],\n","                                 np.array(tokenizer.encode(\"\\\"\"))[1:-1],\n","                                 np.array([tokenizer.mask_token_id]*num_mask_pos),\n","                                 np.array(tokenizer.encode(\"\\\"\"))[1:-1],\n","                                 tokenized_thread[to_start_idx:to_end_idx],\n","                                 np.array(tokenizer.encode(\"\\\"\"))[1:-1],])\n","        yield (prompt, int(rel_type))\n","def generate_prompts(tokenized_thread, comp_type_labels, refers_to_and_type):\n","\n","    length = np.sum(tokenized_thread!=tokenizer.pad_token_id)\n","    spans_lis = get_spans(comp_type_labels, length)\n","    for (rel_type, link_from, link_to) in refers_to_and_type:\n","        \n","        if link_to==0:\n","            continue\n","        \n","        from_start_idx, from_end_idx = spans_lis[link_from-1]\n","        to_start_idx, to_end_idx = spans_lis[link_to-1]\n","        \n","        for prompt, rel in combine_masked_generator(global_flag, tokenized_thread, length, from_end_idx, from_start_idx, to_start_idx, to_end_idx, rel_type):\n","            if tokenizer.model_max_length<prompt.shape[0]:\n","                raise AssertionError(\"Please set max_len in load_dataset so that the sequence length:\", length,\n","                                    \"doesn't execeed the maximum length:\", tokenizer.model_max_length, \n","                                    \"after adding prompt of length:\", prompt.shape[0]-length)\n","            yield (prompt, int(rel))\n","\n"],"execution_count":null,"outputs":[]}]}